include::../../tools/00_0_Lab_Header.adoc[]

== {labname} Lab

In this lab you will explore the various ways to customize pod scheduling. Moving system components to an infra node - and preventing other components from running on that infra node - is a common task that you will practice in this lab.

Most tasks in this lab require a cluster administrator. Setting node selectors and affinity as well as pod affinity and anti-affinity can be done by any user - as long as they know the node labels.

.Goals

* Validate Nodes
* Move Registry, Ingress Controllers and Monitoring to Infra Node
* Use Taints and Tolerations to prevent regular pods to run on Infra Nodes
* Use Pod Affinity and Anti-affinity to place pods
* Update Infra Node MachineSet (Optional)

[[labexercises]]
:numbered:

== Validate Nodes

This lab depends on the completion of the previous labs (Installation, Machine Management). You should have at least 2 worker nodes and 1 infra node available in your cluster.

Remember that your infra node has the label `node-role.kubernetes.io/infra=""`

. Validate that the infra node is available and has the correct labels:
+
[source,text]
----
$ oc get nodes --show-labels|grep infra

infra-1a-s2hqz                Ready    infra,worker         3h13m   v1.18.3+08c38ef   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=4c12g30d,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=regionOne,failure-domain.beta.kubernetes.io/zone=nova,feature.node.kubernetes.io/cpu-cpuid.ADX=true,feature.node.kubernetes.io/cpu-cpuid.AESNI=true,feature.node.kubernetes.io/cpu-cpuid.AVX2=true,feature.node.kubernetes.io/cpu-cpuid.AVX=true,feature.node.kubernetes.io/cpu-cpuid.FMA3=true,feature.node.kubernetes.io/cpu-cpuid.HLE=true,feature.node.kubernetes.io/cpu-cpuid.IBPB=true,feature.node.kubernetes.io/cpu-cpuid.RTM=true,feature.node.kubernetes.io/cpu-cpuid.STIBP=true,feature.node.kubernetes.io/kernel-selinux.enabled=true,feature.node.kubernetes.io/kernel-version.full=4.18.0-193.14.3.el8_2.x86_64,feature.node.kubernetes.io/kernel-version.major=4,feature.node.kubernetes.io/kernel-version.minor=18,feature.node.kubernetes.io/kernel-version.revision=0,feature.node.kubernetes.io/pci-1013.present=true,feature.node.kubernetes.io/pci-1af4.present=true,feature.node.kubernetes.io/system-os_release.ID=rhcos,feature.node.kubernetes.io/system-os_release.VERSION_ID.major=4,feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=5,feature.node.kubernetes.io/system-os_release.VERSION_ID=4.5,kubernetes.io/arch=amd64,kubernetes.io/hostname=infra-1a-s2hqz,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.kubernetes.io/instance-type=4c12g30d,node.openshift.io/os_id=rhcos,topology.kubernetes.io/region=regionOne,topology.kubernetes.io/zone=nova
----

== Move Ingress Controllers, Registry and Monitoring to the Infra Node

It is a best practice to move certain OpenShift components to dedicated nodes that are separate from those running customer workloads. This concept is often referred to as an "infra node". 

These components include the integrated container registry, ingress controllers, the monitoring stack and many optional components like cluster logging.

=== Move the Ingress Controllers

. OpenShift ingress controllers are managed by an Operator called *openshift-ingress-operator*. It lives in the `openshift-ingress-operator` project:
+
[source,text]
----
$ oc get pod -n openshift-ingress-operator

NAME                                READY   STATUS    RESTARTS   AGE
ingress-operator-778fc78f85-gn6q8   2/2     Running   0          167m
----

. The actual default ingress controller instances live in the `openshift-ingress` project (note that they are still called `router` even though the official OpenShift terminoloy is ingress controller):
+
[source,text,options="nowrap"]
----
$ oc get pod -n openshift-ingress -o wide

NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE                       NOMINATED NODE   READINESS GATES
router-default-c87db89d4-xb98r   1/1     Running   0          168m   192.168.47.25   general-purpose-1b-jgjkd   <none>           <none>
router-default-c87db89d4-xsqkq   1/1     Running   0          170m   192.168.47.33   general-purpose-1a-dcfvh   <none>           <none>
----
+
[TIP]
Note how the parameter `-o wide` also shows the nodes that the pods are running on.

. The OpenShift ingress operator monitors a custom resource definition (CRD) called *IngressController*. The IngressController CRD is watched by the router operator. The IngressController object *default* describes for the operator how to create and configure routers. Yours likely looks something like:
+
[source,text]
----
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml

apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2020-08-10T16:18:52Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  managedFields:

[...]

  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "99452"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 014074b1-add8-4a9e-ad0d-eaafc37cdfd6
spec:
  defaultCertificate:
    name: default-ingress-tls
  replicas: 2
status:
  availableReplicas: 2
  conditions:

[...]
----

. Move the ingress controllers to the Infranode.
* What needs to change in the ingress controller object?
+
ifeval::[{show_solution} == true]
.Solution:
Add the nodeSelector` to use `node-role.kubernetes.io/infra: ""`
+
[source,text]
----
$ oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec":{"nodePlacement":{"nodeSelector": {"matchLabels":{"node-role.kubernetes.io/infra":""}}}}}'
----
+
You could also use `oc edit ingresscontroller default -n openshift-ingress-operator`. The final API resource should look like this (unnecessary lines omitted):
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
  replicas: 2
----
endif::[]

. Get list of which nodes are Infranodes:
+
[source,text]
----
$ oc get nodes|grep infra
infra-1a-s2hqz                Ready    infra,worker         3h15m   v1.18.3+08c38ef
----

. Verify that your Ingress Controllers are now running on that node (it may take a minute or two for the old ingress controllers to terminate):
+
[source,text,options="nowrap"]
----
$ oc get pod -o wide -n openshift-ingress

NAME                              READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES
router-default-74d55d6cc7-h66xj   0/1     Pending   0          82s   <none>          <none>           <none>           <none>
router-default-74d55d6cc7-j2scd   1/1     Running   0          96s   192.168.47.13   infra-1a-s2hqz   <none>           <none>
----

. You see there is a problem. One of the ingress controller pods is now in `Pending` state.
. Examine the pending pod and determine out what the problem is. Then fix the problem.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc describe pod router-default-74d55d6cc7-h66xj -n openshift-ingress

[...]
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/6 nodes are available: 3 node(s) didn't have free ports for the requested pod ports, 3 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/6 nodes are available: 3 node(s) didn't have free ports for the requested pod ports, 3 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/6 nodes are available: 2 node(s) didn't have free ports for the requested pod ports, 4 node(s) didn't match node selector.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/6 nodes are available: 1 node(s) didn't have free ports for the requested pod ports, 5 node(s) didn't match node selector.
----
+
The default ingress controller configuration calls for 2 ingress controllers to be deployed on different nodes (using Pod Anti-affinity rules). But you only created one `infra` node and the second ingress controller now has nowhere to run.
+
.Solution 1 - change number of routers:
[source,text]
----
$ oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec":{"replicas": 1}}'
----
+
.Solution 2 - add a second infra node:
[source,text]
----
$ oc scale machineset infra-1a --replicas=2 -n openshift-machine-api
----
endif::[]

=== Move Registry and Monitoring

Now that you know how to set up where certain components should run in an OpenShift cluster, make the appropriate changes to move the registry and monitoring components to your new infra nodes.

Here are some hints:

* The registry operator and operand live in the `openshift-image-registry` project.
* The cluster monitoring components live in the `openshift-monitoring` project.
* The registry placement is controlled with a `nodeSelector` in the `configs.imageregistry.operator.openshift.io/cluster` Custom Resource. This is a cluster scoped Custom Resources as there is only one integrated registry per OpenShift cluster.
* Use `oc explain` to find the correct YAML syntax to add the node selectors.
* The monitoring placement is controlled with a `ConfigMap` called `cluster-monitoring-config`. You can find documentation of this config map in the OpenShift documentation: https://docs.openshift.com/container-platform/4.2/monitoring/cluster-monitoring/configuring-the-monitoring-stack.html#creating-cluster-monitoring-configmap_configuring-monitoring

ifeval::[{show_solution} == true]
==== Solution for Registry

. Examine the current image registry configuration:
+
[source,text]
----
$ oc get configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry -o yaml
----
+
.Sample Output
[source,text]
----
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2020-08-10T16:18:52Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 5
  managedFields:
  
[...]

  name: cluster
  resourceVersion: "99167"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 68334624-41d6-4969-b1ea-51d96d0a091e
spec:
  defaultRoute: true
  httpSecret: 456ee5358f76dd2125587285f315b8837c27f9f9a6de96f452ee9dceb3d15648981b35e504e7399635862263a65afd1611cd0e615d86014e1766d10c0f43053d
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 2
  requests:
    read:
      maxWaitInQueue: 0s
    write:
      maxWaitInQueue: 0s
  rolloutStrategy: Recreate
  routes:
  - hostname: image-registry.apps.cluster-7a2f.blue.osp.opentlc.com
    name: image-registry
  storage:
    pvc:
      claim: image-registry-storage
status:
  conditions:

[...]
----
+
. Add nodeSelector to the `configs.imageregistry.operator.openshift.io/cluster` Custom Resource:
+
[source,text]
----
$ oc patch configs.imageregistry.operator.openshift.io/cluster -n openshift-image-registry --type=merge --patch '{"spec":{"nodeSelector":{"node-role.kubernetes.io/infra":""}}}'
----
+
. Check the registry pods to ensure they redeployed to correct nodes:
+
[source,text]
----
$ oc get pod -o wide -n openshift-image-registry --sort-by=".spec.nodeName"
----

==== Solution for Monitoring

. By default, there is no config map in place to control placement of monitoring components. Create the ConfigMap in the `openshift-monitoring` project:
+
[source,text]
----
$ cat <<EOF > $HOME/monitoring-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
EOF

$ oc create -f $HOME/monitoring-cm.yaml -n openshift-monitoring
----
+
. Validate that the monitoring pods are (re)deploying to the correct node
+
[source,text]
----
$ watch oc get pods -n openshift-monitoring -o wide --sort-by=".spec.nodeName"
----
endif::[]


== Taints and Tolerations

You may have noticed that any workloads in our cluster run on any of three nodes: the two worker nodes or the infra node. This is happening because our infra node is really just another worker node with a specific label.

Now in the real world it is desirable to have only the components run on the infra node that should run on that particular node.

There are two ways of achieving that.

1. Label all worker nodes except the infra node with a custom label. Set a cluster wide node selector to select that label. This will ensure that any pod will only run on regular worker nodes.
2. Add a taint to the infranode. This prevents any pod without matching tolerations from being scheduled onto the infranode. Then set matching tolerations on the infrastructure component pods. Note however that setting just the toleration is not enough - you will also need the node selector to select the infranode. Otherwise the infrastructure components could land on any node (in addition to the infranode).

In this section you will set up a taint on the infra node to influence pod placement. Then you will set up tolerations on the pods to allow the pods to run on the now tainted infranodes.

You can obviously add the taints for the nodes to the machineset, then scale the machinesets down to zero replicas and back up to the number of replicas it had before. This will create new nodes with the appropriate taints. This is the recommended approach.

For the purposes of this lab however you may just use the `oc adm taint` command to add the taints to the nodes. The MachineSet (and Machine) objects do not reconcile existing nodes - therefore this approach is safe. If you were to scale the MachineSets however the new machines would not get the taints.

=== Create sample application, add taints to infra node

. First set up a new project and create a few pods to validate that they are in fact running randomly on all three worker nodes including the infranode.
+
[source,text]
----
$ oc new-project taints
----

. Deploy the little sample application and scale it to 40 replicas. We use 40 replicas to ensure that some pods land on the Infranode - otherwise the scheduler is smart enough to place the pods on the empty nodes and you would not see what you are trying to prove.
+
[source,text]
----
$ oc new-app openshift/hello-openshift:v3.10 --name=nottainted -n taints
$ oc scale deployment nottainted --replicas=40
----

. Find your Infranode
+
[source,text]
----
$ oc get nodes|grep infra
infra-1a-s2hqz             Ready    infra,worker         75m   v1.14.6+c7d2111b9
----

. Examine pod placement
+
[source,text,options="nowrap"]
----
$ oc get pod -n taints -o wide --sort-by=".spec.nodeName"
NAME                  READY   STATUS      RESTARTS   AGE     IP            NODE                       NOMINATED NODE   READINESS GATES
nottainted-1-x9krv    1/1     Running     0          64s     10.130.2.16   general-purpose-1a-55cf7   <none>           <none>

[...]

nottainted-1-pp4v6    1/1     Running     0          64s     10.130.2.22   general-purpose-1a-55cf7   <none>           <none>
nottainted-1-z9k65    1/1     Running     0          64s     10.129.2.26   general-purpose-1b-9klhq   <none>           <none>
nottainted-1-cv2nf    1/1     Running     0          2m2s    10.129.2.20   general-purpose-1b-9klhq   <none>           <none>

[...]

nottainted-1-wm9j5    1/1     Running     0          2m55s   10.129.2.15   general-purpose-1b-9klhq   <none>           <none>
nottainted-1-qb7zc    1/1     Running     0          64s     10.131.2.26   infra-1a-s2hqz             <none>           <none>

[...]

nottainted-1-b8mqf    1/1     Running     0          64s     10.131.2.25   infra-1a-s2hqz             <none>           <none>
----

. Notice that a few of the pods are running on our Infranode which is not what we want.

. Add two taints to the infra node:
* *Key*: infra, *Value*: reserved, *Effect*: NoSchedule
* *Key*: infra, *Value*: reserved, *Effect*: NoExecute
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc adm taint node infra-1a-s2hqz infra=reserved:NoSchedule
$ oc adm taint node infra-1a-s2hqz infra=reserved:NoExecute
----
endif::[]

. Check the pods in your example project again to validate that all pods have been moved to regular worker nodes and no application pods are left on the infra node. Because you added a `NoExecute` taint not only does the scheduler not place new pods - but it actually evicted pods not matching the toleration.
+
[source,text,options="nowrap"]
----
$ oc get pod -n taints -o wide --sort-by=".spec.nodeName"
NAME                  READY   STATUS      RESTARTS   AGE   IP            NODE                       NOMINATED NODE   READINESS GATES
nottainted-1-k75rr    1/1     Running     0          83m   10.130.2.12   general-purpose-1a-55cf7   <none>           <none>

[...]

nottainted-1-zh9nj    1/1     Running     0          82m   10.129.2.27   general-purpose-1b-9klhq   

[...]

nottainted-1-wm9j5    1/1     Running     0          83m   10.129.2.15   general-purpose-1b-9klhq   <none>           <none>
----

. Great, your application now only runs on worker nodes. Since you have proven that this works you may delete the project again.
+
[source,text]
----
$ oc delete project taints
----

=== Allow system components to run on the infra node

. Check your ingress controller, registry pods and monitoring pods.
+
[source,text]
----
$ oc get pod -n openshift-ingress
NAME                              READY   STATUS    RESTARTS   AGE
router-default-5b6b7d5559-6vg8k   0/1     Pending   0          61s
----
+
[source,text]
----
$ oc get pod -n openshift-image-registry

NAME                                              READY   STATUS    RESTARTS   AGE
cluster-image-registry-operator-d6cb8b6bc-t8smx   2/2     Running   0          176m
image-registry-7c55c7bc9-t9lf6                    0/1     Pending   0          17s
image-registry-7c55c7bc9-xxhtf                    0/1     Pending   0          17s
node-ca-m95xw                                     1/1     Running   0          6h27m
node-ca-q7hhf                                     1/1     Running   0          6h27m
node-ca-rsxwg                                     1/1     Running   0          3h19m
node-ca-ts2wp                                     1/1     Running   0          6h27m
node-ca-xvxxp                                     1/1     Running   0          3h21m
node-ca-xxtsc                                     1/1     Running   0          3h20m
----

+
[source,text]
----
$ oc get pod -n openshift-monitoring

NAME                                           READY   STATUS        RESTARTS   AGE
alertmanager-main-0                            0/5     Pending       0          22s
alertmanager-main-1                            0/5     Pending       0          107s
alertmanager-main-2                            0/5     Pending       0          22s
cluster-monitoring-operator-5ddc58cccc-xcggf   2/2     Running       2          176m
grafana-75b69c7c7f-89dp2                       0/2     Pending       0          37s
kube-state-metrics-6c5c797985-52dc6            0/3     Pending       0          37s
node-exporter-7ll85                            2/2     Running       0          6h28m
node-exporter-8bhdn                            2/2     Running       0          3h22m
node-exporter-b5kwr                            2/2     Running       0          6h28m
node-exporter-dl22c                            2/2     Running       0          3h20m
node-exporter-qv6xv                            2/2     Running       0          3h19m
node-exporter-xmzhl                            2/2     Running       0          6h28m
openshift-state-metrics-64fd69bb9c-jlcs6       3/3     Running       0          176m
prometheus-adapter-54b9f6d8dd-975jf            0/1     Pending       0          37s
prometheus-adapter-54b9f6d8dd-wqklj            0/1     Pending       0          37s
prometheus-k8s-0                               0/7     Pending       0          22s
prometheus-k8s-1                               0/7     Pending       0          22s
prometheus-operator-5878cd9d5b-4p2kf           0/2     Pending       0          37s
telemeter-client-c8fbb6d-zqmkb                 0/3     Pending       0          37s
thanos-querier-85b8d58dd-ck6z6                 4/4     Running       0          176m
thanos-querier-85b8d58dd-nj6hn                 4/4     Running       0          176m
----

. You will see that all the components that we previously set a node selector for are now in `Pending` state. This is because the node selector wants them to run on the Infra Node - but the Infra Node now has a taint that repels those pods. And because you added a `NoExecute` taint to the node all pods running on that node got evicted.
+
So let's fix that.
+
. Add matching tolerations to the ingress controllers.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc patch ingresscontroller default -n openshift-ingress-operator --type=merge --patch='{"spec":{"nodePlacement": {"nodeSelector": {"matchLabels": {"node-role.kubernetes.io/infra": ""}},"tolerations": [{"effect":"NoSchedule","key": "infra","value": "reserved"},{"effect":"NoExecute","key": "infra","value": "reserved"}]}}}'
----
endif::[]
. Validate that your ingress controller is now running on the infra node.
+
[source,text,options="nowrap"]
----
$ oc get pod -n openshift-ingress -o wide

NAME                             READY   STATUS    RESTARTS   AGE   IP              NODE             NOMINATED NODE   READINESS GATES
router-default-d457cf65b-cdpj4   1/1     Running   0          21s   192.168.47.13   infra-1a-s2hqz   <none>           <none>
----

. Add matching tolerations to the image registry.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc patch configs.imageregistry.operator.openshift.io cluster --type=merge --patch='{"spec":{"nodeSelector": {"node-role.kubernetes.io/infra": ""},"tolerations": [{"effect":"NoSchedule","key": "infra","value": "reserved"},{"effect":"NoExecute","key": "infra","value": "reserved"}]}}'
----
endif::[]

. Validate that your image registry is now running on the infra node.
+
[source,text,options="nowrap"]
----
$ oc get pod -n openshift-image-registry -o wide

NAME                                              READY   STATUS    RESTARTS   AGE     IP              NODE                          NOMINATED NODE   READINESS GATES
cluster-image-registry-operator-d6cb8b6bc-t8smx   2/2     Running   0          3h      10.128.0.16     cluster-7a2f-bl5bg-master-1   <none>           <none>
image-registry-758d58546b-8qdt9                   1/1     Running   0          13s     10.130.6.32     infra-1a-s2hqz                <none>           <none>
image-registry-758d58546b-rp5lz                   1/1     Running   0          13s     10.130.6.31     infra-1a-s2hqz                <none>           <none>
node-ca-m95xw                                     1/1     Running   0          6h32m   192.168.47.32   cluster-7a2f-bl5bg-master-2   <none>           <none>
node-ca-q7hhf                                     1/1     Running   0          6h32m   192.168.47.19   cluster-7a2f-bl5bg-master-1   <none>           <none>
node-ca-rsxwg                                     1/1     Running   0          3h23m   192.168.47.25   general-purpose-1b-jgjkd      <none>           <none>
node-ca-ts2wp                                     1/1     Running   0          6h32m   192.168.47.17   cluster-7a2f-bl5bg-master-0   <none>           <none>
node-ca-xvxxp                                     1/1     Running   0          3h26m   192.168.47.13   infra-1a-s2hqz                <none>           <none>
node-ca-xxtsc                                     1/1     Running   0          3h24m   192.168.47.33   general-purpose-1a-dcfvh      <none>           <none>
----

. Remember that the placement of the monitoring components was controlled by a config map. Update the config map `cluster-monitoring-config` in project `openshift-monitoring` to add the tolerations to all components.
ifeval::[{show_solution} == true]
+
The Config Map should look like this:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    grafana:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
----
endif::[]
. Validate that the monitoring components are now running on the infranodes. Note that it can take a few (less than 10) minutes for the operator to pick up the config map changes and apply them to the various components.
+
[source,text,options="nowrap"]
----
$ watch oc get pod -n openshift-monitoring -o wide --sort-by=".spec.nodeName"
NAME                                           READY   STATUS    RESTARTS   AGE     IP              NODE                       NOMINATED NODE   READINESS GATES
node-exporter-5df4f                            2/2     Running   2          18h     192.168.47.24   d591-8b7q5-master-0        <none>           <none>
cluster-monitoring-operator-66fc4fbb9f-4rcvn   1/1     Running   1          18h     10.130.0.51     d591-8b7q5-master-0        <none>           <none>
node-exporter-dtrkc                            2/2     Running   2          18h     192.168.47.30   d591-8b7q5-master-1        <none>           <none>
node-exporter-wf6lm                            2/2     Running   2          18h     192.168.47.25   d591-8b7q5-master-2        <none>           <none>
node-exporter-g8dd5                            2/2     Running   0          3h6m    192.168.47.16   general-purpose-1a-55cf7   <none>           <none>
openshift-state-metrics-557985667-pntxf        3/3     Running   0          3h2m    10.129.2.9      general-purpose-1b-9klhq   <none>           <none>
node-exporter-mk2kf                            2/2     Running   0          3h6m    192.168.47.22   general-purpose-1b-9klhq   <none>           <none>
alertmanager-main-0                            3/3     Running   0          6m41s   10.131.2.39     infra-1a-s2hqz             <none>           <none>
node-exporter-br2zr                            2/2     Running   0          174m    192.168.47.27   infra-1a-s2hqz             <none>           <none>
kube-state-metrics-6bdc47964c-tjwpg            3/3     Running   0          7m13s   10.131.2.30     infra-1a-s2hqz             <none>           <none>
grafana-6ff6588f77-cdzbj                       2/2     Running   0          6m58s   10.131.2.35     infra-1a-s2hqz             <none>           <none>
alertmanager-main-2                            3/3     Running   0          7m1s    10.131.2.33     infra-1a-s2hqz             <none>           <none>
alertmanager-main-1                            3/3     Running   0          6m50s   10.131.2.37     infra-1a-s2hqz             <none>           <none>
prometheus-adapter-5865b67cc4-f9bk5            1/1     Running   0          7m      10.131.2.34     infra-1a-s2hqz             <none>           <none>
prometheus-adapter-5865b67cc4-mjtl9            1/1     Running   0          6m51s   10.131.2.36     infra-1a-s2hqz             <none>           <none>
prometheus-k8s-0                               6/6     Running   1          6m38s   10.131.2.40     infra-1a-s2hqz             <none>           <none>
prometheus-k8s-1                               6/6     Running   1          6m50s   10.131.2.38     infra-1a-s2hqz             <none>           <none>
prometheus-operator-686dc76776-lrg8t           1/1     Running   0          7m12s   10.131.2.31     infra-1a-s2hqz             <none>           <none>
telemeter-client-7858cd486f-25m87              3/3     Running   0          7m4s    10.131.2.32     infra-1a-s2hqz             <none>           <none>
----

. Success! All system pods now run on the Infranode - and no other workloads can be scheduled on the infra node.


== Pod Affinity and Anti-affinity

In this section you explore pod affinity and anti-affinity rules. You will create two applications with two replicas each. The first application will have anti-affinity rules to spread the two replicas out over all three nodes. This first application could for example be a Redis cache.

The second application will have anti-affinity rules to spread it out as well. But it will also have affinity rules to bind the pods to the same nodes the the first application is running on. This second application could be a web server that should have each replica on the same node as a redis cache pod.

. In order to show the affinity you will need more than just 2 general purpose nodes in your cluster. Add one node each for zone 1a and 1b.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc scale machineset general-purpose-1a --replicas=2 -n openshift-machine-api
$ oc scale machineset general-purpose-1b --replicas=2 -n openshift-machine-api
----
endif::[]

. Wait until the additional Nodes are Ready.
+
[source,text]
----
$ oc get nodes
NAME                       STATUS   ROLES                AGE     VERSION
d591-8b7q5-master-0        Ready    master               19h     v1.14.6+c7d2111b9
d591-8b7q5-master-1        Ready    master               19h     v1.14.6+c7d2111b9
d591-8b7q5-master-2        Ready    master               19h     v1.14.6+c7d2111b9
general-purpose-1a-55cf7   Ready    general-use,worker   3h15m   v1.14.6+c7d2111b9
general-purpose-1a-q7888   Ready    general-use,worker   49s     v1.14.6+c7d2111b9
general-purpose-1b-9klhq   Ready    general-use,worker   3h15m   v1.14.6+c7d2111b9
general-purpose-1b-qkkgz   Ready    general-use,worker   71s     v1.14.6+c7d2111b9
infra-1a-s2hqz             Ready    infra,worker         3h4m    v1.14.6+c7d2111b9
----

. Create a project for your application
+
[source,text]
----
$ oc new-project scheduler
----

. Deploy two `hello-openshift` applications in the new project:
+
[source,text]
----
$ oc new-app openshift/hello-openshift:v3.10 --name=cache     -n scheduler -lapp=cache
$ oc new-app openshift/hello-openshift:v3.10 --name=webserver -n scheduler -lapp=webserver
----

. Set up the first application (`cache`) with pod anti-affinity rules to spread it out over the available nodes. You can use the label "app=cache" that the `oc new-app` command automatically added to the deployment configuration.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc edit dc cache

[...]
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cache
              topologyKey: kubernetes.io/hostname
      containers:
[...]
----
endif::[]

. Now scale the `cache` application to two replicas. You are only using two replicas so that you can later examine pod affinity.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc scale dc cache --replicas=2
----
endif::[]

. Verify that the two pods are all running on different nodes:
+
[source,text]
----
$ oc get pod -o wide|grep Running
cache-2-bdg6h        1/1     Running     0          16s     10.130.2.35   general-purpose-1a-55cf7   <none>           <none>
cache-2-cm2fx        1/1     Running     0          16s     10.129.2.40   general-purpose-1b-9klhq   <none>           <none>
----
+
[NOTE]
Your pods may land on different nodes. The important part is that they should not land on the same node.

. Now set up the `webserver` deployment config with pod anti-affinity rules for itself - and an affinity rule for the `cache` application. You can use the application labels again (`app=cache` and `app=webserver`) to set up the correct affinity and anti-affinity rules.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc edit dc webserver

[...]
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cache
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webserver
              topologyKey: kubernetes.io/hostname
            weight: 100
      containers:
[...]
----
endif::[]

. Now scale the `webserver` application to two replicas.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc scale dc webserver --replicas=2
----
endif::[]

. Validate that the pods are all running on the correct nodes. You should see a cache and associated webserver pod on the same two nodes. Which means that one node will not have any pods scheduled.
+
[source,text,options="nowrap"]
----
$ oc get pod -o wide|grep Running
cache-2-bdg6h        1/1     Running     0          60s     10.130.2.35   general-purpose-1a-55cf7   <none>           <none>
cache-2-cm2fx        1/1     Running     0          60s     10.129.2.40   general-purpose-1b-9klhq   <none>           <none>
webserver-2-2bj2d    1/1     Running     0          13s     10.130.2.36   general-purpose-1a-55cf7   <none>           <none>
webserver-2-tdlf9    1/1     Running     0          13s     10.129.2.41   general-purpose-1b-9klhq   <none>           <none>
----

. Delete the two applications
+
[source,text]
----
$ oc delete all -lapp=cache
$ oc delete all -lapp=webserver
----

. Remove the Scheduler project:
+
[source,text]
----
$ oc delete project scheduler
----

. Remove the two additional Nodes that we created to demonstrate the affinity effects:
+
[source,text]
----
$ oc scale machineset general-purpose-1a --replicas=1 -n openshift-machine-api
$ oc scale machineset general-purpose-1b --replicas=1 -n openshift-machine-api
----

== Update the Infra Node MachineSet (Optional)

Previously you just used `oc adm taint node` to add the infra taints to the infra node. On clusters where the nodes manage via MachineSets is generally a good idea to change the MachineSets that manage the Machines and therefore Nodes. Otherwise any change to the MachineSet will not propagate to nodes. This may become a problem if you need to scale the Infra node MachineSet - and new nodes will not automatically get the correct taints.

. Update the Infra node MachineSet to include the two taints:
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc patch machineset infra-1a -n openshift-machine-api --type='merge' --patch='{"spec": {"template": {"spec": {"taints": [{"key": "infra","value": "reserved","effect": "NoSchedule"},{"key": "infra","value": "reserved","effect": "NoExecute"}]}}}}'
----

endif::[]

. Scale the MachineSet down to zero.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc scale machineset infra-1a --replicas=0 -n openshift-machine-api
----
endif::[]

. Wait until the Machine has been deleted then scale the MachineSet back to 1 replica.
ifeval::[{show_solution} == true]
+
[source,text]
----
$ oc scale machineset infra-1a --replicas=1 -n openshift-machine-api
----
endif::[]

. Validate that the new node still has the taints.
+
[source,text]
----
$ oc describe node $(oc get nodes|awk '/infra/{print $1}')|grep infra=reserved:No.*
Taints:             infra=reserved:NoExecute
                    infra=reserved:NoSchedule
----

== Summary

In this lab you explored the various ways to place pods on the _right_ nodes in an OpenShift cluster. You used this knowledge to ensure all OpenShift infrastructure components are running on a dedicated Infranode - and that no other workloads get scheduled on this Infranode.

You also explored how Pod Affinity and Anti-Affinity rules affect the placement of pods relative to each other.
