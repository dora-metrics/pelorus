include::../../tools/00_0_Lab_Header.adoc[]

== {labname} Lab

.Goals
In this lab, you configure your cluster's nodes using the built-in mechanisms in OpenShift^(R)^ 4.

* Explore Nodes, Machines, and MachineSets
** Add and remove nodes to the cluster
** Change the labels for new nodes
* Set up nodes for infrastructure services
* Explore Cluster Autoscaling

[[labexercises]]
:numbered:

== Overview

In the previous lab, you deployed an OpenShift 4 cluster using the UPI method. Not only did you perform a full UPI install, you did it offline - meaning all of the container images that were required for the installation were pulled, or mirrored, locally beforehand. This is a common requirement in many enterprise environments. 

However just because you were "disconnected" and couldn't pull the OpenShift container images from the Internet doesn't mean you can't take advantage of some of the amazing management features OpenShift 4 offers. If you recall from the previous module, there were different components of the OpenShift cluster that could be considered connected or disconnected. Installation was one of them and _Machine Management_ was another. In this module, you'll use your cluster in "connected" mode to let it begin managing certain aspects of itself - namely its nodes.

=== Nodes, Machines, and MachineSets
Nodes, Machines, and MachineSets are all different types of objects in OpenShift. They are very closely related to one another, but they all serve a different purpose.

Nodes run Pods. It is that simple. That is their only job. You can do things to Nodes to attract certain types of Pods of repel other types of Pods, some of which will be covered in this module and later.

OpenShift 4 introduces some additional concepts and objects - Machines and MachineSets. Machines can be thought of as a representation of the underlying "hardware". Notice that hardware is used in quotes here because Machines can only represent virtual machines or instances. When you look at a Machine object, you'll see specific information pertaining to the instance it represents as well as the cloud or IaaS provider.

MachineSets are the next layer up from Machines. MachineSets are separate objects and define a type of Machine they are responsible for as well as the number of replicas that should be deployed. You can compare this to the functionality provided by a ReplicaSet for Pods. MachineSets contain the entire definition of the Machine as a template. This means that a MachineSet can create many copies of a specific Machine, but you would need multiple MachineSets if you have multiple types of Machines. For instance, if you have a MachineSet that provisions Machines with GPUs in them, you'd have to create a different MachineSet if you wanted to provision Machines without GPUs.

For these three things, think of it like this. MachineSets create Machines. Machines provision (virtual) instances that bootstrap and join the cluster as Nodes. Nodes run Pods.

So far, that just sounds like a lot of additional things you have to manage alongside nodes. Don't worry - OpenShift has a solution that ties all of this together.

=== Machine API

The *Machine API Operator* is the OpenShift component that does all of the work here. Some Operators are simple. If you think back to the first module, you'll remember that an Operator may just watch for an instance, or _CR_, of a certain `kind` and then do something - like create or modify a `Deployment`. They use native Kubernetes objects and controllers. This is fine for many use cases, but what happens when you need more logic that doesn't come from these native functions? The Machine API Operator as a whole not only watches for the kinds of objects it is responsible for, but also has custom controllers that will perform necessary actions. Some of these actions include:

* Creating new virtual machines
** Different requirements depending on cloud provider
* Deleting unecessary virtual machines
* Keeps track of Node <> Machine relationships
* Ensures the correct number of Machines are deployed based on MachineSet replicas
* Ensures correct settings (such as labels and taints) are applied from Machine > Node

In addition to doing all of these things for you, the Machine API only requires you to define what you want deployed using a _declaritive_ approach. This means that you tell the Machine API what you want the state to be and it will go and do it for you. If you had to define the steps that it would take, you'd be using an _imperative_ approach and the learning curve would be steeper. 

So, what exactly do you need to provide? You'll work on that in the following lab, but to give you an idea, you can use the `oc explain` command to understand exactly what you might be expected to provide (this command does not seem to work on OpenShift 4.3 and 4.4 at the moment):

[source,sh]
----
$ oc explain MachineSet.spec --recursive=true
----

.Sample Output
[source,sh]
----
KIND:     MachineSet
VERSION:  machine.openshift.io/v1beta1

RESOURCE: spec <Object>

DESCRIPTION:
     / [MachineSetSpec] MachineSetSpec defines the desired state of MachineSet

FIELDS:
   deletePolicy	<string>
   minReadySeconds	<integer>
   replicas	<integer>
   selector	<Object>
      matchExpressions	<[]Object>
         key	<string>
         operator	<string>
         values	<[]string>
      matchLabels	<map[string]string>
   template	<Object>
      metadata	<Object>

[...]
----

The *FIELDS* section in the above output is what you can provide. Based on this, the Machine API will go and provision, deprovision, or configure your Machines and Nodes.

== Lab Tips

* *Do not run as root*. Continue to do these labs as the same user - your OpenTLC username.
* Watch for *Sample Output*. If you see this, know that you will need to substitue values from *your* environment. For instance, this lab guide references a node named `0138-6mqfj-worker-0`. Your worker will be named something different.
* Some *Sample Output* in this lab guide may have been shortened for readability. Your output may not match the lab guide, but the important parts will.

[TIP]
There is a rather extensive FAQ available at https://github.com/openshift/machine-api-operator/blob/master/FAQ.md

== Explore MachineSets and Machines

In this lab, you will configure new `MachineSets`, which will result in new `Machines` and `Nodes` being created. Everything you create in this lab will be used and enhanced throughout the remainder of the course. 

All of the `Machine` and `MachineSet` objects are namespaced and part of the `openshift-machine-api` namespace. `Node` objects are cluster wide.

. If not already connected, ssh to your `bastion`. Use the connection information provided in the provisioning email.
+
NOTE: As in the previous module, do everything as your OpenTLC user. Do not run as root. You don't need to and if you followed the requirements in the previous module you will be set up to do all of the remaining modules as the non-root user.

. Retrieve the list of `nodes` currently deployed in your OpenShift cluster. Your names will be different, but you should have three `master` and two `worker` nodes.
+
[source,sh]
----
$ oc get nodes
----
+
.Sample Output
[source,sh]
----
NAME                          STATUS   ROLES    AGE   VERSION
cluster-7a2f-bl5bg-master-0   Ready    master   31m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-1   Ready    master   31m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-2   Ready    master   31m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-0   Ready    worker   20m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-1   Ready    worker   20m   v1.18.3+08c38ef
----

. You can see additional details about these nodes using the `oc describe` command. This command is something that you will use often, both for infrastructure components and application workloads. Look at one of your `worker` nodes.
+
TIP: If you don't like the output of `oc describe`, you can use the `oc get` command with different output options - `oc get node XXXX -o yaml` would get you detailed information about a node in `yaml` format.
+
[source,sh]
----
$ oc describe node cluster-7a2f-bl5bg-worker-0
----
+
.Sample Output
[source,sh]
----
Name:               cluster-7a2f-bl5bg-worker-0
Roles:              worker <1>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=4c16g30d
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=regionOne
                    failure-domain.beta.kubernetes.io/zone=nova
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=cluster-7a2f-bl5bg-worker-0
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/worker=
                    node.kubernetes.io/instance-type=4c16g30d
                    node.openshift.io/os_id=rhcos
                    topology.kubernetes.io/region=regionOne
                    topology.kubernetes.io/zone=nova
Annotations:        machineconfiguration.openshift.io/currentConfig: rendered-worker-d51af9b4a4aa575316855cb509cfc4ac
                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-d51af9b4a4aa575316855cb509cfc4ac
                    machineconfiguration.openshift.io/reason:
                    machineconfiguration.openshift.io/state: Done
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Aug 2020 12:25:21 -0400
Taints:             <none> <2>
Unschedulable:      false
Lease:
  HolderIdentity:  cluster-7a2f-bl5bg-worker-0
  AcquireTime:     <unset>
  RenewTime:       Mon, 10 Aug 2020 12:45:41 -0400
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 10 Aug 2020 12:42:02 -0400   Mon, 10 Aug 2020 12:25:21 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 10 Aug 2020 12:42:02 -0400   Mon, 10 Aug 2020 12:25:21 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 10 Aug 2020 12:42:02 -0400   Mon, 10 Aug 2020 12:25:21 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 10 Aug 2020 12:42:02 -0400   Mon, 10 Aug 2020 12:26:01 -0400   KubeletReady                 kubelet is posting ready status
Addresses: <3>
  Hostname:    cluster-7a2f-bl5bg-worker-0
  InternalIP:  192.168.47.29
Capacity:
  attachable-volumes-cinder:  256
  cpu:                        4
  ephemeral-storage:          30905324Ki
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     16419136Ki
  pods:                       250
Allocatable:
  attachable-volumes-cinder:  256
  cpu:                        3500m
  ephemeral-storage:          27408604728
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     15268160Ki
  pods:                       250
System Info:
  Machine ID:                              17ae2f1a601346d690d876bafa4334ed
  System UUID:                             17ae2f1a-6013-46d6-90d8-76bafa4334ed
  Boot ID:                                 ebf41557-ba8b-4112-b659-4c8399464a87
  Kernel Version:                          4.18.0-193.14.3.el8_2.x86_64
  OS Image:                                Red Hat Enterprise Linux CoreOS 45.82.202008010929-0 (Ootpa)
  Operating System:                        linux
  Architecture:                            amd64
  Container Runtime Version:               cri-o://1.18.3-8.rhaos4.5.gitbefe37e.el8
  Kubelet Version:                         v1.18.3+08c38ef
  Kube-Proxy Version:                      v1.18.3+08c38ef
PodCIDR:                                   10.128.4.0/24
PodCIDRs:                                  10.128.4.0/24
ProviderID:                                openstack://17ae2f1a-6013-46d6-90d8-76bafa4334ed
Non-terminated Pods:                       (27 in total) <4>
  Namespace                                Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                                ----                                          ------------  ----------  ---------------  -------------  ---
  openshift-cluster-node-tuning-operator   tuned-qw9jf                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         20m
  openshift-cluster-storage-operator       csi-snapshot-controller-74c95cc477-wwdsq      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         19m
  openshift-dns                            dns-default-njsl9                             65m (1%)      0 (0%)      110Mi (0%)       512Mi (3%)     20m
  openshift-image-registry                 image-registry-689cb67849-fxzjn               100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         19m
  openshift-image-registry                 node-ca-s8xn6                                 10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         20m
  openshift-ingress                        router-default-678fcc5674-mmmkv               100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         26m
  openshift-kube-storage-version-migrator  migrator-84d8d67946-9r2kw                     100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         30m
  openshift-machine-config-operator        machine-config-daemon-4ct9m                   40m (1%)      0 (0%)      100Mi (0%)       0 (0%)         20m
  openshift-marketplace                    certified-operators-68d8bc5645-7ncw9          10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         26m
  openshift-marketplace                    community-operators-575ff64669-77l9w          10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         26m
  openshift-marketplace                    redhat-marketplace-69b694bd49-67cvh           10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         26m
  openshift-marketplace                    redhat-operators-77699c7d8b-sr6f7             10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         26m
  openshift-monitoring                     alertmanager-main-0                           8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         19m
  openshift-monitoring                     alertmanager-main-2                           8m (0%)       0 (0%)      260Mi (1%)       0 (0%)         19m
  openshift-monitoring                     kube-state-metrics-54f5d785f8-w8rmb           4m (0%)       0 (0%)      120Mi (0%)       0 (0%)         26m
  openshift-monitoring                     node-exporter-5mm76                           9m (0%)       0 (0%)      210Mi (1%)       0 (0%)         20m
  openshift-monitoring                     openshift-state-metrics-64fd69bb9c-vjkr8      3m (0%)       0 (0%)      190Mi (1%)       0 (0%)         26m
  openshift-monitoring                     prometheus-adapter-757d9c6f5-xx6wf            1m (0%)       0 (0%)      25Mi (0%)        0 (0%)         21m
  openshift-monitoring                     prometheus-k8s-1                              76m (2%)      0 (0%)      1184Mi (7%)      0 (0%)         19m
  openshift-monitoring                     telemeter-client-5dc68f466d-q8jt8             3m (0%)       0 (0%)      20Mi (0%)        0 (0%)         26m
  openshift-monitoring                     thanos-querier-85b8d58dd-l5kqc                8m (0%)       0 (0%)      72Mi (0%)        0 (0%)         19m
  openshift-multus                         multus-zph9d                                  10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         20m
  openshift-openstack-infra                coredns-cluster-7a2f-bl5bg-worker-0           100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         19m
  openshift-openstack-infra                keepalived-cluster-7a2f-bl5bg-worker-0        100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         19m
  openshift-openstack-infra                mdns-publisher-cluster-7a2f-bl5bg-worker-0    100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         19m
  openshift-sdn                            ovs-2vl27                                     100m (2%)     0 (0%)      400Mi (2%)       0 (0%)         20m
  openshift-sdn                            sdn-lf2tp                                     100m (2%)     0 (0%)      200Mi (1%)       0 (0%)         20m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                   Requests      Limits
  --------                   --------      ------
  cpu                        1105m (31%)   0 (0%)
  memory                     5123Mi (34%)  512Mi (3%)
  ephemeral-storage          0 (0%)        0 (0%)
  hugepages-1Gi              0 (0%)        0 (0%)
  hugepages-2Mi              0 (0%)        0 (0%)
  attachable-volumes-cinder  0             0
Events:                      <none>
----
+
<1> The `Roles` list is based off of the `node-role.kubernetes.io` label. You can have multiple roles assigned to a node.
<2> There are no `taints` assigned to a `worker` node by default. If you were to inspect a `master` node, you'd find a `taint`. These concepts will be covered in the _Scheduler_ module.
<3> These attributes would match what you see in your IaaS provider, in this case OpenStack. You can use the `openstack server show XXXX` command to compare.
<4> All of the `Pods` currently running on this node. Since you have not deployed any application workloads, these are all OpenShift cluster workloads. Some of these will be moved to dedicated nodes in later modules.

. Retrieve the list of `Machines` currently deployed in your cluster. You should have none deployed. This is because you removed the `manifests` that would have created the `master` nodes and you set the number of replicas to 0 in the manifest for the worker `MachineSet`.
+
[source,sh]
----
$ oc get machines -n openshift-machine-api
----
+
.Sample Output
[source,sh]
----
No resources found.
----

. Retrieve the list of `MachineSets` currently deployed in your cluster.
+
[source,sh]
----
$ oc get machineset -n openshift-machine-api
----
+
.Sample Output
[source,sh]
----
NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-7a2f-bl5bg-worker   0         0                             34m
----
[#machinesetdetails]
+
This worker `MachineSet` was created for you during the installation because you left the `manifest` that defines this `MachineSet` in place during the last module. Normally, in a UPI installation you'd remove this as well. It was left, in this case, to give you something to start with in this module.

. Look at the details for this `MachineSet`.
+
[source,sh]
----
$ oc get machinesets cluster-7a2f-bl5bg-worker -o yaml -n openshift-machine-api
----
+
.Sample Output
[source,sh,options="nowrap"]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  creationTimestamp: "2020-05-10T05:22:38Z"
  generation: 1
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: cluster-7a2f-bl5bg-worker <1>
  namespace: openshift-machine-api
  resourceVersion: "4388"
  selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/cluster-7a2f-bl5bg-worker
  uid: 1b447959-982c-4ae0-bda9-396a2701d644
spec:
  replicas: 0 <2>
  selector:
    matchLabels: <3>
      machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
      machine.openshift.io/cluster-api-machineset: cluster-7a2f-bl5bg-worker
  template: <4>
    metadata:
      creationTimestamp: null
      labels: <3>
        machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-7a2f-bl5bg-worker
    spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials <5>
            namespace: openshift-machine-api
          flavor: 4c16g30d
          image: cluster-7a2f-bl5bg-rhcos
          kind: OpenstackProviderSpec
          metadata:
            creationTimestamp: null
          networks:
          - filter: {}
            subnets:
            - filter: <6>
                name: cluster-7a2f-bl5bg-nodes
                tags: openshiftClusterID=cluster-7a2f-bl5bg
          securityGroups:
          - filter: {}
            name: cluster-7a2f-bl5bg-worker
          serverMetadata:
            Name: cluster-7a2f-bl5bg-worker
            openshiftClusterID: cluster-7a2f-bl5bg
          tags:
          - openshiftClusterID=cluster-7a2f-bl5bg
          trunk: true
          userDataSecret:
            name: worker-user-data
status:
  observedGeneration: 1
  replicas: 0
----
+
<1> This is the name of your `MachineSet`. Keep it named something descriptive.
<2> The number of replicas you want this `MachineSet` to deploy. The Machine API will make sure there is always this number of `Machines` deployed.
<3> These are the labels that the controller will look for on `Machines` in order to match them to a `MachineSet`. All of the `matchLabels` must match on the respective `Machine`.
<4> This is the template that will be used to create a `Machine`. This means everything from this point on will be applied to the `Machine` objects created by this `MachineSet`.
<5> The credentials that the OpenShift uses to communicate with the IaaS or cloud provider. These were created automatically by the installer based on the credentials you used at that time.
<6> Filters can be used to help find the exact component you need from the IaaS or cloud provider. All of these filters must match, so make sure your list is accurate.

TIP: The above is an example for the OpenStack provider. Consult the documentation for more details on what is required for other providers.

Now that you have seen these objects and understand what they are doing, what will happen if you scale up the existing `MachineSet`?

=== Scale a MachineSet

In OpenShift 4, the preferred method to add and remove `nodes` to a cluster is using `Machines` and `MachineSets`. You describe the spec of the `Machine` you want to provision in a `MachineSet`. In addition to the spec, you also include other information such as the number of replicas you want to deploy. This only works when deploying OpenShift onto an IaaS or cloud provider that is supported. If you were to deploy on bare metal, this machine management functionality would not work. There has to be some supported API for the `machine-api` to talk to and created your desired state. 

You can manipulate these objects either through the command line using the `oc` client or by using the Web UI. While the steps are different (typing vs clicking), you are editing the same objects using either method. This lab guide will assume you are using the command line.

. Still on your `bastion`, as your OpenTLC user, retrieve your list of `MachineSets`.
+
[source,sh]
----
$ oc get machineset -n openshift-machine-api
----
+
.Sample Output
[source,sh]
----
NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-7a2f-bl5bg-worker   0         0                             2d2h
----

. You currently have this `MachineSet` configured with zero replicas. Change this now to 1 replica.
+
[source,sh]
----
$ oc scale machineset cluster-7a2f-bl5bg-worker --replicas=1 -n openshift-machine-api
----
+
.Sample Output
[source,sh]
----
machineset.machine.openshift.io/cluster-7a2f-bl5bg-worker scaled
----

. Your `MachineSet` should be scaled to 1 and you should have a `Machine` deploying now. 
.. Verify machineset scale-up.
+
[source,sh]
----
$ oc get machineset -n openshift-machine-api
----
+
.Sample Output
[source,sh]
----
NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-7a2f-bl5bg-worker   1         1                             2d2h
----

.. List machines.
+
[source,bash]
----
$ oc get machine -n openshift-machine-api
----
+
.Sample Output:
[source,text]
----
NAME                              STATE   TYPE   REGION   ZONE   AGE
cluster-7a2f-bl5bg-worker-5prxd   ERROR                          60s
----

. Unfortunately, while your `MachineSet` did add another `Machine`, clearly something is wrong. Take a look at the `Machine` object using `oc describe` and see if it tells you the problem.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc describe machine cluster-7a2f-bl5bg-worker-5prxd -n openshift-machine-api*

Name:         cluster-7a2f-bl5bg-worker-5prxd
Namespace:    openshift-machine-api
Labels:       machine.openshift.io/cluster-api-cluster=cluster-7a2f-bl5bg
              machine.openshift.io/cluster-api-machine-role=worker
              machine.openshift.io/cluster-api-machine-type=worker
              machine.openshift.io/cluster-api-machineset=cluster-7a2f-bl5bg-worker
Annotations:  machine.openshift.io/instance-state: ERROR
API Version:  machine.openshift.io/v1beta1
Kind:         Machine
...
Events:
  Type     Reason        Age                  From                  Message
  ----     ------        ----                 ----                  -------
  Warning  FailedCreate  14s (x14 over 104s)  openstack_controller  CreateError
----

. You have an error, so no new instance is being created in the cloud provider - OpenStack. You'll have to figure out what is causing the error, though. To do this, you will need to look at some of the `Machine API` controller logs.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc get pods -n openshift-machine-api*

NAME                                        READY   STATUS    RESTARTS   AGE
cluster-autoscaler-operator-6d5cfb9-xfmrj   2/2     Running   0          31m
machine-api-controllers-7df9c8cd49-nqhzq    4/4     Running   0          32m
machine-api-operator-6696c8489b-l4npz       2/2     Running   0          33m
----
+
* The `cluster-autoscaler-operator` is the Operator responsible for deploying and managing the cluster autoscaler controller. You have not deployed this yet.
* The `machine-api-operator` is the Operator responsible for deploying and managing the `machine-api-controllers`.
* The `machine-api-controllers` are where all of the work to interact with the cloud or IaaS provider takes place.

. There are 4 containers in the `machine-api-controllers-XXXX` pod. 
+
[source,text,subs="{markup-in-source}"]
----
$ *oc get pod machine-api-controllers-7df9c8cd49-nqhzq -o json -n openshift-machine-api | jq -r .spec.containers[].name*

controller-manager
machine-controller
nodelink-controller
machine-healthcheck-controller
----
+
* The `controller-manager` is the "master" controller and has the big picture.
* The `machine-controller` reconciles desired state for `Machines` by ensuring that instances with a desired config exist in a given cloud provider.
* The `nodelink-controller` reconciles desired state of `Machines` by matching IP addresses of machine objects with IP addresses of node objects and annotates nodes with a special machine annotation containing the machine name. The annotation is interpreted and sets the corresponding nodeRef field of each relevant machine.
* The `machine-healthcheck-controller` is responsible for monitoring the health of machines and replacing when necessary. You can create your own machine health check.

. From the descriptions above, you can see that the `machine-controller` is the most likely of these to be experiencing a problem when a new `Machine` is being created. Check the logs of this container.
+
TIP: When pulling the logs from a `Pod` that has multiple containers, you need to use the `-c <container-name>` option in the `oc logs` command.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc logs machine-api-controllers-7df9c8cd49-nqhzq -c machine-controller -n openshift-machine-api | grep -i worker*

I1121 04:42:49.654930       1 controller.go:133] Reconciling Machine "cluster-7a2f-bl5bg-worker-5prxd"
I1121 04:42:49.655043       1 controller.go:304] Machine "cluster-7a2f-bl5bg-worker-5prxd" in namespace "openshift-machine-api" doesn't specify "cluster.k8s.io/cluster-name" label, assuming nil cluster
I1121 04:42:50.432733       1 controller.go:253] Reconciling machine object cluster-7a2f-bl5bg-worker-5prxd triggers idempotent create.
E1121 04:42:52.979196       1 actuator.go:470] Machine error cluster-7a2f-bl5bg-worker-5prxd: error creating Openstack instance: No network was found or provided. Please check your machine configuration and try again
W1121 04:42:52.979220       1 controller.go:255] Failed to create machine "cluster-7a2f-bl5bg-worker-5prxd": error creating Openstack instance: No network was found or provided. Please check your machine configuration and try again
----

. Scale your `MachineSet` back to zero and confirm that the `Machine` causing the error is removed. You should have no `Machines` objects now.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc scale machineset cluster-7a2f-bl5bg-worker --replicas=0 -n openshift-machine-api*
$ *oc get machines -n openshift-machine-api*

No resources found.
----

. Create two `yaml` files on your `bastion` with the definition of two new `MachineSets` using the following requirements and tips.
* The `MachineSets` should be named as follows. The labels should be applied to the `nodes` that are provisioned through the `MachineSets`.
** *general-purpose-1a*
*** failure-domain.beta.kubernetes.io/region: "regionOne"
*** failure-domain.beta.kubernetes.io/zone: "nova"
*** node-role.kubernetes.io/general-use: ""
** *general-purpose-1b*
*** failure-domain.beta.kubernetes.io/region: "regionOne"
*** failure-domain.beta.kubernetes.io/zone: "nova"
*** node-role.kubernetes.io/general-use: ""
* There should be 0 *replicas* of each `MachineSet` - you will scale up later
* They should use a flavor named `4c12g30d`
* You can export the existing `MachineSet` to make a copy and edit locally by running `oc get machineset <machineset-name> -o yaml -n openshift-machine-api > $HOME/worker-ms-1a.yaml`
* You can get a list of OpenStack subnets by running `openstack subnet list`
** *DO NOT* use the `external` subnet.
* You can get a list of OpenStack security groups by running `openstack security group list`
** The `Machines` are workers, so they should be assigned an appropriately named security group
* Be careful that your `filters` don't look for attributes that do not exist
+

ifeval::[{show_solution == true}]

. Your `general-purpose-1a` yaml file should look similar to this.
+
NOTE: These are sample values. Yours will be different.
+
[source,subs="{markup-in-source}"]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: general-purpose-1a
  namespace: openshift-machine-api
spec:
  replicas: 0
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
      machine.openshift.io/cluster-api-machineset: general-purpose-1a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: general-purpose-1a
    spec:
      metadata:
        labels:
          failure-domain.beta.kubernetes.io/region: "regionOne"
          failure-domain.beta.kubernetes.io/zone: "nova"
          node-role.kubernetes.io/general-use: ""
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: 4c12g30d
          image: rhcos-ocp45
          kind: OpenstackProviderSpec
          networks:
          - filter: {}
            subnets:
            - filter:
                name: 7a2f-ocp-subnet
          securityGroups:
          - filter: {}
            name: 7a2f-worker_sg
          serverMetadata:
            Name: cluster-7a2f-bl5bg-worker
            openshiftClusterID: cluster-7a2f-bl5bg
          tags:
          - openshiftClusterID=cluster-7a2f-bl5bg
          trunk: false
          userDataSecret:
            name: worker-user-data
----

. Your `general-purpose-1b` yaml file should look similar to the above, with the `MachineSet` name being different.

. If you are having trouble creating this, a template has been added to your bastion for you. Run the following `ansible` command to generate a `yaml` file that you can use to create the `MachineSet`.
+
[source,options="nowrap"]
----
$ for i in 1a 1b
do 
ansible localhost -m template -a "src='$HOME/resources/general-ms.yaml.j2' dest='$HOME/general-purpose-$i.yaml'" -e msid=$i
done
----

endif::[]
+
TIP: TAKE YOUR TIME. Get this right. Use the <<machinesetdetails,description>> of the `MachineSet` if you need help.

. Once you are satisified with your `MachineSet` definitions, create both of the objects in OpenShift.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc create -f <your-ms-yaml.yaml> -n openshift-machine-api*
$ *oc get machineset -n openshift-machine-api*

NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-7a2f-bl5bg-worker   0         0                             74m
general-purpose-1a          0         0                             4m33s
general-purpose-1b          0         0                             3m40s
----

. Scale your `MachineSets` up to 1. Running these commands will update the spec of the `MachineSet` to define 1 replica for each. You could also do this by running `oc edit` and manually updating the object.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc scale machineset general-purpose-1a --replicas=1 -n openshift-machine-api*
$ *oc scale machineset general-purpose-1b --replicas=1 -n openshift-machine-api*
----

. Validate that your `Machines` are being created. These new `Machines` will take several minutes to actually be created in the cloud or IaaS provider. Be patient.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc get machines -n openshift-machine-api*

NAME                       PHASE     TYPE       REGION      ZONE   AGE
general-purpose-1a-vzw7v   Running   4c12g30d   regionOne   nova   4m49s
general-purpose-1b-6z7v6   Running   4c12g30d   regionOne   nova   3m55s
----

TIP: You can see your new instances being created by running the `openstack server list` command.

. Validate that these new `Machines` have been added to your cluster as `nodes`. This will take approximately 5-10 minutes.
+
[source,text,subs="{markup-in-source}"]
.Sample Output
----
$ *oc get nodes*

NAME                          STATUS   ROLES                AGE   VERSION
cluster-7a2f-bl5bg-master-0   Ready    master               75m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-1   Ready    master               74m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-2   Ready    master               75m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-0   Ready    worker               64m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-1   Ready    worker               64m   v1.18.3+08c38ef
general-purpose-1a-vzw7v      Ready    general-use,worker   73s   v1.18.3+08c38ef
general-purpose-1b-6z7v6      Ready    general-use,worker   41s   v1.18.3+08c38ef
----

You have now completed this section of the lab. You have created custom `MachineSets` and scaled them up so that new `Machines` are created and `nodes` added to the cluster. What was different about the end of this process compared to the last module when you manually added worker nodes? You didn't have to approve the `CSRs` this time. Since the OpenShift `machine-api` created these, it allowed their admission into the cluster.

=== Cleanup Workers

You now have your OpenShift cluster configured and able to provision and deprovision its own workers. Take some time now to remove the workers that you manually added in the previous module. What are some of the things you need to do in order to prepare a node for removal and actually remove it? Remember, these were not created by the `machine API`, so you will have to take steps to manually remove them.

. Remove the 2 workers you created manually in the previous module. Use the following tips to accomplish this task.
* Cordon and drain your nodes
** Cordon your nodes so that no new `Pods` are scheduled
** Drain your nodes to evict all running `Pods`
** Use the OpenShift documentation for help: https://docs.openshift.com/container-platform/4.3/nodes/nodes/nodes-nodes-working.html
* Delete the node from OpenShift

ifeval::[{show_solution == true}]

. Cordon one of your manual workers to ensure no new `Pods` are scheduled to it.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc adm cordon cluster-7a2f-bl5bg-worker-0*

node/cluster-7a2f-bl5bg-worker-0 cordoned
----

. Now that no new `Pods` can be scheduled to this node, you must evict, or drain, all of the existing `Pods`. On these nodes, you have both `Pods` backed by a replication controller and `Pods` that are deployed as part of a `DaemonSet`. Make sure all `Pods` are removed.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc adm drain cluster-7a2f-bl5bg-worker-0 --ignore-daemonsets --delete-local-data --force=true*

node/cluster-7a2f-bl5bg-worker-0 already cordoned
WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-qw9jf, openshift-dns/dns-default-njsl9, openshift-image-registry/node-ca-s8xn6, openshift-machine-config-operator/machine-config-daemon-4ct9m, openshift-monitoring/node-exporter-5mm76, openshift-multus/multus-zph9d, openshift-sdn/ovs-2vl27, openshift-sdn/sdn-lf2tp
evicting pod openshift-marketplace/redhat-marketplace-69b694bd49-67cvh
evicting pod openshift-cluster-storage-operator/csi-snapshot-controller-74c95cc477-wwdsq
evicting pod openshift-image-registry/image-registry-689cb67849-fxzjn
evicting pod openshift-ingress/router-default-678fcc5674-mmmkv
evicting pod openshift-kube-storage-version-migrator/migrator-84d8d67946-9r2kw
evicting pod openshift-marketplace/certified-operators-68d8bc5645-7ncw9
evicting pod openshift-marketplace/community-operators-575ff64669-77l9w
evicting pod openshift-monitoring/kube-state-metrics-54f5d785f8-w8rmb
evicting pod openshift-marketplace/redhat-operators-77699c7d8b-sr6f7
evicting pod openshift-monitoring/alertmanager-main-0
evicting pod openshift-monitoring/alertmanager-main-2
evicting pod openshift-monitoring/prometheus-adapter-757d9c6f5-xx6wf
evicting pod openshift-monitoring/openshift-state-metrics-64fd69bb9c-vjkr8
evicting pod openshift-monitoring/prometheus-k8s-1
evicting pod openshift-monitoring/telemeter-client-5dc68f466d-q8jt8
evicting pod openshift-monitoring/thanos-querier-85b8d58dd-l5kqc
I0810 13:29:51.610837    8121 request.go:621] Throttling request took 1.198860318s, request: POST:https://api.cluster-7a2f.blue.osp.opentlc.com:6443/api/v1/namespaces/openshift-monitoring/pods/thanos-querier-85b8d58dd-l5kqc/eviction
pod/alertmanager-main-0 evicted
pod/redhat-operators-77699c7d8b-sr6f7 evicted
pod/community-operators-575ff64669-77l9w evicted
pod/image-registry-689cb67849-fxzjn evicted
pod/migrator-84d8d67946-9r2kw evicted
pod/kube-state-metrics-54f5d785f8-w8rmb evicted
pod/alertmanager-main-2 evicted
pod/redhat-marketplace-69b694bd49-67cvh evicted
pod/csi-snapshot-controller-74c95cc477-wwdsq evicted
pod/certified-operators-68d8bc5645-7ncw9 evicted
I0810 13:30:03.679811    8121 request.go:621] Throttling request took 1.104530357s, request: GET:https://api.cluster-7a2f.blue.osp.opentlc.com:6443/api/v1/namespaces/openshift-monitoring/pods/telemeter-client-5dc68f466d-q8jt8
pod/telemeter-client-5dc68f466d-q8jt8 evicted
pod/thanos-querier-85b8d58dd-l5kqc evicted
pod/openshift-state-metrics-64fd69bb9c-vjkr8 evicted
pod/prometheus-adapter-757d9c6f5-xx6wf evicted
pod/prometheus-k8s-1 evicted
pod/router-default-678fcc5674-mmmkv evicted
node/cluster-7a2f-bl5bg-worker-0 evicted
----
+
* The `--ignore-daemonsets` option will ensure that `Pods` backed by a `DaemonSet`, which cannot be evicted, don't prevent the drain activity on the node from being successful.
* The `--delete-local-data` option will ensure that `Pods` using local storage are evicted with their storage cleaned up.
* The `--force=true` option will ensure that any `Pods` not backed by a replication controller - called _bare Pods_ - are properly evicted as well.

. Notice in the output of the previous command that it noted that worker had already been cordoned. The `oc adm drain` command will cordon a node for you first, if you have not already done so, before it proceeds with evicting the `Pods`. Do this on the remaining worker that was manually created.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc adm drain cluster-7a2f-bl5bg-worker-1 --ignore-daemonsets --delete-local-data --force=true*

node/cluster-7a2f-bl5bg-worker-1 cordoned
WARNING: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-7x552, openshift-dns/dns-default-s4fnw, openshift-image-registry/node-ca-dmktk, openshift-machine-config-operator/machine-config-daemon-wv4lr, openshift-monitoring/node-exporter-s828d, openshift-multus/multus-5jjq8, openshift-sdn/ovs-bpsch, openshift-sdn/sdn-n2zrn
evicting pod openshift-ingress/router-default-678fcc5674-drvrc
evicting pod openshift-cluster-storage-operator/csi-snapshot-controller-operator-7674cf997f-5rngh
evicting pod openshift-image-registry/image-registry-689cb67849-6wvv8
evicting pod openshift-monitoring/prometheus-adapter-757d9c6f5-bdfzx
evicting pod openshift-monitoring/alertmanager-main-1
evicting pod openshift-monitoring/grafana-6854f5454d-7rw5b
evicting pod openshift-monitoring/thanos-querier-85b8d58dd-xtdmc
evicting pod openshift-monitoring/prometheus-k8s-0
evicting pod openshift-service-catalog-removed/openshift-service-catalog-apiserver-remover-l5njk
evicting pod openshift-service-catalog-removed/openshift-service-catalog-controller-manager-remover-8n79p
pod/openshift-service-catalog-apiserver-remover-l5njk evicted
pod/openshift-service-catalog-controller-manager-remover-8n79p evicted
I0810 13:31:23.095362    8141 request.go:621] Throttling request took 1.0355273s, request: GET:https://api.cluster-7a2f.blue.osp.opentlc.com:6443/api/v1/namespaces/openshift-ingress/pods/router-default-678fcc5674-drvrc
pod/image-registry-689cb67849-6wvv8 evicted
pod/thanos-querier-85b8d58dd-xtdmc evicted
pod/grafana-6854f5454d-7rw5b evicted
pod/prometheus-adapter-757d9c6f5-bdfzx evicted
pod/csi-snapshot-controller-operator-7674cf997f-5rngh evicted
pod/alertmanager-main-1 evicted
pod/prometheus-k8s-0 evicted
pod/router-default-678fcc5674-drvrc evicted
node/cluster-7a2f-bl5bg-worker-1 evicted
----

. Verify that both of these nodes have been removed from the schduler pool.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get nodes*

NAME                          STATUS                     ROLES                AGE     VERSION
cluster-7a2f-bl5bg-master-0   Ready                      master               80m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-1   Ready                      master               79m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-2   Ready                      master               79m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-0   Ready,SchedulingDisabled   worker               68m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-worker-1   Ready,SchedulingDisabled   worker               68m     v1.18.3+08c38ef
general-purpose-1a-vzw7v      Ready                      general-use,worker   6m4s    v1.18.3+08c38ef
general-purpose-1b-6z7v6      Ready                      general-use,worker   5m32s   v1.18.3+08c38ef
----

. Delete the nodes.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc delete node cluster-7a2f-bl5bg-worker-0 cluster-7a2f-bl5bg-worker-1*

node "cluster-7a2f-bl5bg-worker-0" deleted
node "cluster-7a2f-bl5bg-worker-1" deleted
----

. Verify that the nodes are removed.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get node*

NAME                          STATUS   ROLES                AGE     VERSION
cluster-7a2f-bl5bg-master-0   Ready    master               80m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-1   Ready    master               80m     v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-2   Ready    master               80m     v1.18.3+08c38ef
general-purpose-1a-vzw7v      Ready    general-use,worker   6m25s   v1.18.3+08c38ef
general-purpose-1b-6z7v6      Ready    general-use,worker   5m53s   v1.18.3+08c38ef
----

endif::[]

. With your nodes deleted from OpenShift, make sure you delete the VM in OpenStack as well.
+
[source,text,options="nowrap"]
----
$ openstack server list --name $INFRA_ID-worker -f value -c ID | xargs openstack server delete
$ openstack server list -c ID -c Name -c Status

+--------------------------------------+-----------------------------+--------+
| ID                                   | Name                        | Status |
+--------------------------------------+-----------------------------+--------+
| ee83047d-4fd2-4f50-8d21-b0711d83a040 | general-purpose-1b-6z7v6    | ACTIVE |
| cb999132-18aa-445b-9e67-21191322a27b | general-purpose-1a-vzw7v    | ACTIVE |
| 95b15516-2481-4d4e-8827-8f354df2fedd | cluster-7a2f-bl5bg-master-2 | ACTIVE |
| ae0c5728-c747-46fd-9825-5f7e13a1a824 | cluster-7a2f-bl5bg-master-1 | ACTIVE |
| 01f952ea-49c9-4c1a-888b-d7eb61c75c92 | cluster-7a2f-bl5bg-master-0 | ACTIVE |
| 5bee17ea-c36d-4c0b-943a-5f3608cf36b6 | utilityvm                   | ACTIVE |
| c75aa939-048b-4ba3-9d89-6d2adacbba00 | bastion                     | ACTIVE |
+--------------------------------------+-----------------------------+--------+
----

Notice that you did nothing to the `master` nodes in this lab. There is no reason to touch them. They are not currently managed the same way a `worker` node would be, meaning you can't add and remove them at will. Treat your `master` nodes like a faithful pet.

== Add an Infrastructure Node

Even though, from a subscription management perspective, there is no concept of an "Infrastructure Node" in OpenShift 4, there is still a good reason to have nodes created just to run infrastructure services. This ensures that application workloads do not mix and potentially cause problems with the infrastructure workloads that are operating a cluster. An infrastructure node is nothing more than another worker node in a cluster with some additional labels. These labels help influence certain Pods in the cluster to run on them. 

The built-in label for these nodes is `node-role.kubernetes.io/infra=""`. Applying this to a node will add `infra` to its roles when you run the `oc get nodes` command. You could also add additional labels that allow even more specialized workloads to target that node. For instance, if you had nodes that were meant to only run logging components, you could add an additional arbitrarily defined label, such as `infra-role=logging`. As long as you provide this label to the Pod spec or Operator of the logging components, the scheduler would ensure the Pods run on the right nodes. This will be covered much more in the Scheduler module.

. Create a new `MachineSet` to run infrastructure workloads. Use the following requirements.
* Create the `MachineSet` with a name of `infra-1a`
* The number of `replicas` should be set to 1
* Apply the following labels to the nodes that will be created:
** `node-role.kubernetes.io/infra: ""`
** `failure-domain.beta.kubernetes.io/region: regionOne`
** `failure-domain.beta.kubernetes.io/zone: nova`
* Use a flavor of `4c12g30d`

ifeval::[{show_solution == true}]

. Start by taking an export of one of your existing `MachineSets` that you know works.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get machineset general-purpose-1a -o yaml -n openshift-machine-api > $HOME/infra-1a.yaml*
----

. Modify the `$HOME/infra-1a.yaml` file to meet the requirements listed above. Your file contents should be similar to this:
+
[source,text,subs="{markup-in-source}"]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: infra-1a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
      machine.openshift.io/cluster-api-machineset: infra-1a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-7a2f-bl5bg
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: infra-1a
    spec:
      metadata:
        labels:
          failure-domain.beta.kubernetes.io/region: "regionOne"
          failure-domain.beta.kubernetes.io/zone: "nova"
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          apiVersion: openstackproviderconfig.openshift.io/v1alpha1
          cloudName: openstack
          cloudsSecret:
            name: openstack-cloud-credentials
            namespace: openshift-machine-api
          flavor: 4c12g30d
          image: rhcos-ocp45
          kind: OpenstackProviderSpec
          metadata:
            creationTimestamp: null
          networks:
          - filter: {}
            subnets:
            - filter:
                name: ded1-ocp-subnet
          securityGroups:
          - filter: {}
            name: ded1-worker_sg
          serverMetadata:
            Name: cluster-7a2f-bl5bg-worker
            openshiftClusterID: cluster-7a2f-bl5bg
          tags:
          - openshiftClusterID=cluster-7a2f-bl5bg
          trunk: false
          userDataSecret:
            name: worker-user-data
----

. Create the `MachineSet` object in OpenShift.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc create -f infra-1a.yaml*

machineset.machine.openshift.io/infra-1a created
----

. Wait for the new `node` to become available. You can watch both the `Machine` and `node` list with the following commands.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc get machines -n openshift-machine-api -w*
$ *oc get nodes -w*
----

endif::[]

. Verify your new infrastructure node is only with the correct roles.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc get nodes*

NAME                          STATUS   ROLES                AGE    VERSION
cluster-7a2f-bl5bg-master-0   Ready    master               148m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-1   Ready    master               147m   v1.18.3+08c38ef
cluster-7a2f-bl5bg-master-2   Ready    master               147m   v1.18.3+08c38ef
general-purpose-1a-vzw7v      Ready    general-use,worker   74m    v1.18.3+08c38ef
general-purpose-1b-6z7v6      Ready    general-use,worker   73m    v1.18.3+08c38ef
infra-1a-tghzh                Ready    infra,worker         59s    v1.18.3+08c38ef
----

And with that, you are complete with this section. You have added a third `MachineSet`, which is deploying a `node` labeled for use by infrastructure workloads. You won't move anything to this new node now, but you will use it in later module when you target specific workloads to run on nodes like this.

== Autoscaling

You've learned how to create the resources and configurations that allow OpenShift to provision and deprovision its own machines. However, this process still required user input. You had to either modify the `MachineSet` object or run the `oc scale` command to increase the number of replicas. In this section, you will explore the capabilities of the cluster autoscaler.

=== Cluster Autoscaler

The cluster autoscaler has a seemingly simple job. It adjusts the size of the cluster by adding and removing nodes to meet the demand of the current deployment needs. When you define "deployment needs," you mean all of the `Pods` that need a node in the cluster to run on. There are, of course, different types of `Pods` that run in a cluster with different levels of priority that can affect when and how much a cluster is automatically scaled. `Pod` priorities will be discussed in much more detail in the scheduler module. Setting all of this up may sound difficult, but it is not. While there is certainly complexity to the cluster autoscaler with how it decides when it needs to do something, it is surprisingly simple for you to configure. This is largely (again) thanks to the Operator model.

The Operator that manages the configuration and deployment of the cluster autoscaler is called the cluster autoscaler operator. This Operator watches for custom resources that are created with a kind of `ClusterAutoscaler`. When it sees one of those - and there can be only one as a cluster scoped resource - it will do the following:

. Deploy the cluster autoscaler controller if not already deployed
. Update the configuration of the cluster autoscaler controller if already deployed

From there, the controller takes over and will manage the available capacity in your cluster. It does this by watching for `Pods` that cannot currently be scheduled due to insufficient resources. This may be because the cluster is out of CPU or memory. It may also be because a certain type of node is needed - one with a GPU, for instance. Or, it may be as simple as needing an arbitrary node with a certain label to accomodate a workload. Imagine a situation where you have a development team running something like a `Horizontal Pod Autoscaler`. This is a feature you will learn about in more detail later, but it is something that can automatically add `Pods` to a deployment based on load. If these `Pods` are being added, you want to make sure that your cluster has the capacity needed to adequetly run them without someone needing to be logged in and watching utilization 24/7.

With the cluster autoscaler, you can control the overall capacity and configuration of the cluster autoscaler and then you let it do what it was designed to do. Before you move onto the labs, there is another piece to the overall cluster autoscaler that is required.

=== Machine Autoscaler

A cluster autoscaler cannot directly increase the number of replicas of, or scale up, a `MachineSet`. It needs an intermediate resource called a `MachineAutoscaler`. This is another object that you must create and assign to a `MachineSet`. Only those `MachineSets` with a `MachineAutoscaler` assigned to them will participate in autoscaling activities in the cluster.

The `MachineAutoscaler` is a namespace scoped resource, unlike the `ClusterAutoscaler`. It must be created in the `openshift-machine-api` namespace. This is the same namespace that you have been creating your `Machine` and `MachineSet` resources in throughout this module. The `MachineAutoscaler` is much simpler to define and really is focused on three things:

. The `scaleTargetRef` that it is responsible for is always the name of the `MachineSet`
. The maximum number of nodes for the `MachineSet`
. The minimum number of nodes for the `MachineSet`

Only `MachineSets` with a `MachineAutoscaler` associated with them will participate in cluster autoscaling. If you think you have set everything in your `ClusterAutoscaler` up correctly and your cluster is not autoscaling then looking at your `MachineAutoscalers` is a good place to start.

=== Create a Cluster Autoscaler

Since the cluster autoscaler is managed by an Operator, how do you create the configuration? You need to create a `custom resource` that defines the correct apiVersion, kind, and spec. Once that is created, the cluster autoscaler Operator will take action.

Now that the explanation and recap are complete, it is time for you to start doing things to your cluster. In this section, you will:

* Create an appropriate number of `MachineAutoscalers`
* Create an appropriate number of `Cluster Autoscalers`
* Deploy a workload that will force the cluster autoscaler to increase capacity in the cluster
* Cleanup (this is important!)

. Log into your `bastion` as your OpenTLC user. Do not do these things as root.

. Before you start, take a minute to explore what the options are for the resources you will be creating. You can use the `oc explain` command to identify the options that the Operator will know how to do something with.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc explain clusterautoscaler.spec --recursive=true*

KIND:     ClusterAutoscaler
VERSION:  autoscaling.openshift.io/v1

RESOURCE: spec <Object>

DESCRIPTION:
     Desired state of ClusterAutoscaler resource

FIELDS:
   balanceSimilarNodeGroups	<boolean>
   ignoreDaemonsetsUtilization	<boolean>
   maxPodGracePeriod	<integer>
   podPriorityThreshold	<integer>
   resourceLimits	<Object>
      cores	<Object>
         max	<integer>
         min	<integer>
      gpus	<[]Object>
         max	<integer>
         min	<integer>
         type	<string>
      maxNodesTotal	<integer>
      memory	<Object>
         max	<integer>
         min	<integer>
   scaleDown	<Object>
      delayAfterAdd	<string>
      delayAfterDelete	<string>
      delayAfterFailure	<string>
      enabled	<boolean>
      unneededTime	<string>
   skipNodesWithLocalStorage	<boolean>
----

. If you need more details on something, you can continue to use `oc explain` and expose more information.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc explain clusterautoscaler.spec.balanceSimilarNodeGroups*

KIND:     ClusterAutoscaler
VERSION:  autoscaling.openshift.io/v1

FIELD:    balanceSimilarNodeGroups <boolean>

DESCRIPTION:
     BalanceSimilarNodeGroups enables/disables the
     `--balance-similar-node-groups` cluster-autocaler feature. This feature
     will automatically identify node groups with the same instance type and the
     same set of labels and try to keep the respective sizes of those node
     groups balanced.
----

. Change your context to the `openshift-machine-api` project. This is a function of the `oc` client that will save you from adding a `-n <project-name>` to all of the commands you run. You will use this frequently when you only want to work with objects in a specific namespace.
+
[source,subs="{markup-in-source}"]
----
$ *oc project openshift-machine-api*
----

. Identify the `MachineSets` you have to work with.
+
[source,subs="{markup-in-source}"]
----
$ *oc get machineset*

NAME                        DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-7a2f-bl5bg-worker   0         0                             148m
general-purpose-1a          1         1         1       1           78m
general-purpose-1b          1         1         1       1           77m
infra-1a                    1         1         1       1           3m50s
----

. Deploy a cluster autoscaling solution that meets these requirements:
** The `ClusterAutoscaler` is named `default`
** The autoscale policy should apply to all default pod priorities
** Only the `general-purpose-1a` and `general-purpose-1b` MachineSets are autoscaled
** The `infra-1a` MachineSet is *not* autoscaled
** The maximum number of nodes in each `MachineSet` is 4.
** The maximum CPU and memory is 48 and 156, respectively
** The minimum CPU and memory is 20 and 80, respectively
** Scale down functionality should be enabled with values set to `5m`
** Use the link:https://docs.openshift.com/container-platform/4.4/machine_management/applying-autoscaling.html[Autoscaling documentation^] to help you

ifeval::[{show_solution == true}]

. Create the `MachineAutoscaler` for `general-purpose-1a`.
+
[source,text,subs="{markup-in-source}"]
----
$ *echo "
apiVersion: autoscaling.openshift.io/v1beta1
kind: MachineAutoscaler
metadata:
  name: ma-general-purpose-1a
  namespace: openshift-machine-api
spec:
  minReplicas: 1
  maxReplicas: 4
  scaleTargetRef:
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet
    name: general-purpose-1a" | oc create -f - -n openshift-machine-api*
----

. Create the `MachineAutoscaler` for `general-purpose-1b`.
+
[source,text,subs="{markup-in-source}"]
----
$ *echo "
apiVersion: autoscaling.openshift.io/v1beta1
kind: MachineAutoscaler
metadata:
  name: ma-general-purpose-1b
  namespace: openshift-machine-api
spec:
  minReplicas: 1
  maxReplicas: 4
  scaleTargetRef:
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet
    name: general-purpose-1b" | oc create -f - -n openshift-machine-api*
----

. Verify that both of your `MachineAutoscalers` were created correctly and have the correct *REF NAME*.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get machineautoscaler*

NAME                    REF KIND     REF NAME             MIN   MAX   AGE
ma-general-purpose-1a   MachineSet   general-purpose-1a   1     4     13s
ma-general-purpose-1b   MachineSet   general-purpose-1b   1     4     6s
----

. With both `MachineAutoscalers` created, you must now define your `ClusterAutoscaler`.
+
[source,text,subs="{markup-in-source}"]
----
$ *echo "
apiVersion: autoscaling.openshift.io/v1
kind: ClusterAutoscaler
metadata:
  name: default
spec:
  balanceSimilarNodeGroups: true
  podPriorityThreshold: -10
  resourceLimits:
    maxNodesTotal: 12
    cores:
      min: 20
      max: 48
    memory:
      min: 80
      max: 156
  scaleDown:
    enabled: true
    delayAfterAdd: 5m
    delayAfterDelete: 5m
    delayAfterFailure: 5m
    unneededTime: 60s" | oc create -f -*
----

. Verify that your `ClusterAutoscaler` was created correctly.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc describe clusterautoscaler default*

Name:         default
Namespace:
Labels:       <none>
Annotations:  <none>
API Version:  autoscaling.openshift.io/v1
Kind:         ClusterAutoscaler
Metadata:
  Creation Timestamp:  2020-08-10T18:43:29Z
  Generation:          1
  Managed Fields:

[...]

  Resource Version:  61967
  Self Link:         /apis/autoscaling.openshift.io/v1/clusterautoscalers/default
  UID:               6482da15-6dfc-4bf0-b861-a518fa2d1ea7
Spec:
  Balance Similar Node Groups:  true
  Pod Priority Threshold:       -10
  Resource Limits:
    Cores:
      Max:            48
      Min:            20
    Max Nodes Total:  12
    Memory:
      Max:  156
      Min:  80
  Scale Down:
    Delay After Add:      5m
    Delay After Delete:   5m
    Delay After Failure:  5m
    Enabled:              true
    Unneeded Time:        60s
Events:                   <none>
----
endif::[]

. With your autoscaler resources created, you can see that there are annotations on your `MachineSets` now. This is how the cluster autoscaler determines which `MachineSets` it can use. There may be other annotations as well - but you should see the three listed in the example below.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get machinesets -o yaml | grep annotations -A 3*

    annotations:
      autoscaling.openshift.io/machineautoscaler: openshift-machine-api/ma-general-purpose-1a
      machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
      machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
--
    annotations:
      autoscaling.openshift.io/machineautoscaler: openshift-machine-api/ma-general-purpose-1b
      machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
      machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
----
+
NOTE: You only see 2 sets of results, even though you have more `MachineSets`, because you only created `MachineAutoscalers` for 2 of them.

. Once your `ClusterAutoscaler` resource is correctly created, you can see that there is a new Pod in this project as well. The cluster autoscaler Operator is responsible for watching for new `ClusterAutoscaler` custom resources being created. When it sees one of those, it either deploys or configures the cluster autoscaler controller.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc get pods*

NAME                                          READY   STATUS    RESTARTS   AGE
cluster-autoscaler-default-84bfb94db8-2s698   1/1     Running   0          2m3s
cluster-autoscaler-operator-6d5cfb9-xfmrj     2/2     Running   0          145m
machine-api-controllers-7df9c8cd49-nqhzq      4/4     Running   0          146m
machine-api-operator-6696c8489b-l4npz         2/2     Running   0          147m
----

. Check the logs for *your* newly deployed controller. You should see that is is watching two different `MachineSets` and there is currently no need for it to make any changes. Leave this running in "follow" mode. You will use it in the next section.
+
[source,text,options="nowrap",subs="{markup-in-source}"]
----
$ *oc logs -f cluster-autoscaler-default-84bfb94db8-2s698*

...
I1122 03:13:00.739486       1 static_autoscaler.go:318] No unschedulable pods
I1122 03:13:00.739660       1 utils.go:552] Skipping general-purpose-1a-c55lj - node group min size reached
I1122 03:13:00.739722       1 utils.go:552] Skipping general-purpose-1b-ml9l2 - node group min size reached
I1122 03:13:00.740129       1 scale_down.go:706] No candidates for scale down
----

Your cluster autoscaler is now successfully deployed and waiting to do some work.

=== Create load for autoscaler

You have created an autoscaler, but it is not doing anything. Remember what needs to happen for the autoscaler to begin doing things? There must be deployments, specifically Pods, that need to be scheduled to nodes but can't be due to resource availability. Recall the log messages from the previous section that stated there were "No unschedulable pods". In this section, you will test your new `ClusterAutoscaler` by generating a workload.

. Open a new SSH session to your `bastion`. You should leave your other SSH session intact so that you can observe the logs easily.

. Create a new project for the new workload.
+
[source,text,subs="{markup-in-source}"]
----
$ *oc new-project work-queue*
----
+
TIP: When you create a new project using the `oc` client, it will automatically switch your context. This is similar to what you did earlier in this lab with the `oc project` command. 

. Now, Create the new workload. You will be using a `Job` workload. This will start a container to run a command or task, in this case sleeping, a specific number of times. In this case, it will try to start 50 of these Pods. However, look at their CPU & memory requirements and see that there is not enough capacity in the cluster to run these without scaling up.
+
[source,bash]
----
$ echo 'apiVersion: batch/v1
kind: Job
metadata:
  generateName: work-queue-
spec:
  template:
    spec:
      containers:
      - name: work
        image: busybox
        command: ["sleep",  "300"]
        resources:
          requests:
            memory: 500Mi
            cpu: 300m
      restartPolicy: Never
      nodeSelector:
        node-role.kubernetes.io/general-use: ""
  parallelism: 50
  completions: 50' | oc create -f - -n work-queue
----

. In your other SSH window, you'll see some activity in the cluster autoscaler logs. Spcifically, look for the recommendations that will be implemented.
+
[source,text,options="nowrap"]
----
I1202 23:49:41.066452       1 scale_up.go:422] Best option to resize: openshift-machine-api/general-purpose-1a
I1202 23:49:41.066469       1 scale_up.go:426] Estimated 5 nodes needed in openshift-machine-api/general-purpose-1a
I1202 23:49:41.066551       1 scale_up.go:521] Splitting scale-up between 2 similar node groups: {openshift-machine-api/general-purpose-1a, openshift-machine-api/general-purpose-1b}
I1202 23:49:41.066562       1 scale_up.go:529] Final scale-up plan: [{openshift-machine-api/general-purpose-1a 1->4 (max: 4)} {openshift-machine-api/general-purpose-1b 1->3 (max: 4)}]
I1202 23:49:41.066607       1 scale_up.go:686] Scale-up: setting group openshift-machine-api/general-purpose-1a size to 4
I1202 23:49:41.079834       1 scale_up.go:686] Scale-up: setting group openshift-machine-api/general-purpose-1b size to 3
----

. Over the next several minutes, you can watch new `Machines` be created. Each `Machine` and the respective `node` will take 5-6 minutes to join the cluster.
+
[source,text,subs="{markup-in-source}"]
----
$ *watch -n 10 "oc get machines -n openshift-machine-api"*
----

. When you see that the machines are being provisioned observe that OpenShift Nodes are also being created.
+
[source,text,subs="{markup-in-source}"]
----
$ *watch -n 10 "oc get nodes"*
----

. Each job in the work queue will run for 5 minutes. As jobs complete, the cluster eventually shrinks in size as machines are removed. Let this process finish so that your cluster reverts to its original size.

. If you do not want to wait for this to finish, move on to the next section.

== Cleanup

When you are finished with these exercises, you need to clean up. If you do not do this, your cluster autoscaler will keep running and may scale down `nodes` you are using in later labs. You'll wonder why if you don't finish this quick section.

Also, if you don't do these tasks, later labs will not work.

WARNING: MAKE SURE YOU FINISH THIS WHOLE SECTION. LATER LABS WILL NOT WORK IF YOU DON'T.

. Delete the `work-queue` project. This will delete both the job and any remaining `Pods` you have in that project.
+
[source,subs="{markup-in-source}"]
----
$ *oc delete project work-queue*
----

WARNING: *Wait for the cluster autoscaler to finish scaling down the Machines. This will take time. Be patient.*

. Once the cluster is back to normal size (2 "general-use" worker nodes), you can delete the `MachineAutoscalers`. 
+
[source,options="nowrap",subs="{markup-in-source}"]
----
$ *oc delete machineautoscaler ma-general-purpose-1a ma-general-purpose-1b -n openshift-machine-api*
----

. Delete the cluster autoscaler.
+
[source,subs="{markup-in-source}"]
----
$ *oc delete clusterautoscaler default*
----

. Update the labels on your Nodes. This is going to do a few things:
.. It will change the `failure-domain.beta.kubernetes.io/zone` key to a value of `nova` on both the Machine and MachineSet objects. When you update the MachineSet, the change is not appliied automatically to existing Machines and Nodes. However, when you update a Machine like this, the updated label is automatically and immediately added to its Node.
.. It will change the `failure-domain.beta.kubernetes.io/region` key to a value of `regionOne`.
.. It will delete either of these existing labels from the Nodes and the Machine API will automatically apply the new ones. The reason you delete these last is so that you clear out any unwanted labels for this key:value pair. The Machine API will only add labels to a Node as defined in a Machine object - it will not remove them automatically. Setup your default project as openshift-machine-api.
+
[source,text,options="nowrap"]
----
$ for i in $(oc get machineset -n openshift-machine-api -o name);do oc patch $i --type=merge -p '{"spec": {"template": {"spec": {"metadata": {"labels": {"failure-domain.beta.kubernetes.io/zone": "nova"}}}}}}';done
$ for i in $(oc get machineset -n openshift-machine-api -o name);do oc patch $i --type=merge -p '{"spec": {"template": {"spec": {"metadata": {"labels": {"failure-domain.beta.kubernetes.io/region": "regionOne"}}}}}}';done
----

////
# With 4.5 the following is no longer necessary because the MachineSet now reconciles...

$ for i in $(oc get node -l node-role.kubernetes.io/worker -o name);do oc label $i failure-domain.beta.kubernetes.io/zone-;done
$ for i in $(oc get node -l node-role.kubernetes.io/worker -o name);do oc label $i failure-domain.beta.kubernetes.io/region-;done
$ for i in $(oc get machine -n openshift-machine-api -o name);do oc patch $i --type=merge -p '{"spec": {"metadata": {"labels": {"failure-domain.beta.kubernetes.io/zone": "nova"}}}}' -n openshift-machine-api;done
$ for i in $(oc get machine -n openshift-machine-api -o name);do oc patch $i --type=merge -p '{"spec": {"metadata": {"labels": {"failure-domain.beta.kubernetes.io/region": "regionOne"}}}}' -n openshift-machine-api;done
////

For your reference, you are performing that last step due to the way the at OpenShift currently integrates with OpenStack Cinder storage. There is currently only one _Availability Zone_ in the OpenStack environment. When the a new PV is requested from the Cinder storage provider, it will pass the `failure-domain.beta.kubernetes.io/zone` value you have set for the Nodes. Since that `zone` doesn't exist, your PVs will fail to create and your PVCs will remain stuck in pending in later labs. 

Pay attention to this kind of thing when you are integrating with a cloud provider so that you do not have unexpected side affects by arbitrarily changing a label that the underlying cloud provider requires to be accurate for functionality.

== Solving and Resetting (beta)

Use the steps here to either solve or reset this lab.
Solving the lab will set up everything that you would have done in this lab.

There is no reset option for this lab.

. Run the solver using FTL.
+
[source,options="nowrap"]
----
$ ansible-playbook /opt/ftl-repo-clone/courses/ocp4_advanced_deployment/lab_04_1/solve_lab.yml
----
