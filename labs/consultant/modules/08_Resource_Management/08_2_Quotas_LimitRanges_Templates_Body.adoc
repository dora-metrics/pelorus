include::../../tools/00_0_Lab_Header.adoc[]

== {labname} Lab

.Scenario

MitziCom is starting production, and they are very concerned that they may run out of resources. In order to address these concerns, you need to set quotas and limits on the system.

MitziCom has decided to deploy Red Hat^(R)^ 3scale API Management in their Red Hat OpenShift^(R)^ Container Platform environment to evaluate its performance impact on the cluster. They have asked you to create a template to deploy 3scale API Management in a repeatable manner, taking into account its resource requirements. You need to edit the template that deploys 3scale API Management in order for users to use it be able to deploy it with enough resources and appropriate limits to function properly.

.Goals

* Create quotas for the cluster
* Import the 3scale AMP template to set up 3scale API Management
* Deploy the system
* Edit the template to launch 3scale properly


// WK: No include of lab setup. This builds on completion of the previous lab.
[[labexercises]]
:numbered:

== Create Quotas for Cluster

. Switch to the `system:admin` user to set a quota on `andrew`:
+
[source]
----
$ oc login -u system:admin
----

. Create a limited cluster resource quota that sets the following hard limits:
* `pods=25`
* `requests.memory=6Gi`
* `requests.cpu=5`
* `limits.cpu=25`
* `limits.memory=40Gi`
* `configmaps=25`
* `persistentvolumeclaims=25`
* `services=25`
* `project-annotation-selector` for the `andrew` user

ifeval::[{show_solution} == true]
+
[source]
----
$ export OCP_USERNAME=andrew
$ oc create clusterquota clusterquota-${OCP_USERNAME} \
 --project-annotation-selector=openshift.io/requester=$OCP_USERNAME \
 --hard pods=25 \
 --hard requests.memory=6Gi \
 --hard requests.cpu=5 \
 --hard limits.cpu=25  \
 --hard limits.memory=40Gi \
 --hard configmaps=25 \
 --hard persistentvolumeclaims=25  \
 --hard services=25
----

endif::[]
. View the cluster resource quota:
+
[source]
----
$ oc get clusterresourcequota
NAME                  LABEL SELECTOR   ANNOTATION SELECTOR
clusterquota-andrew   <none>           map[openshift.io/requester:andrew]
----
. Describe the cluster resource quota. Note that you wont have any values shown yet, because the requester has no projects yet.
+
[source]
----
$ oc describe clusterresourcequota clusterquota-andrew
Name:		clusterquota-andrew
Created:	18 seconds ago
Labels:		<none>
Annotations:	<none>
Namespace Selector: []
Label Selector:
AnnotationSelector: map[openshift.io/requester:andrew]
Resource	Used	Hard
--------	----	----
----

[TIP]
For some reason the `oc describe` command does not display the actual resources. To see that you created the clusterresourcequota correctly use `oc get clusterresourcequota clusterquota-andrew -o yaml`.

== Download 3scale Templates from Internet

. Get the 3scale AMP template:
+
[source]
----
$ wget https://raw.githubusercontent.com/3scale/3scale-amp-openshift-templates/2.1.0-GA/amp/amp.yml
----

== Deploy 3scale System

=== Import 3scale AMP Template

. Switch to the `andrew` user:
+
[source]
----
$ oc login -u andrew -p openshift
----

. Create a new project for 3scale API Management:
+
[source]
----
$ oc new-project 3scale
----

. Create the 3scale AMP template and, optionally, the APIcast template:
+
[source]
----
$ oc create -f amp.yml
$ oc get template
NAME      DESCRIPTION   PARAMETERS     OBJECTS
system                  23 (1 blank)   49
----

. Create a 3scale Admin Portal with `oc new-app`:
+
[source,options="nowrap"]
----
$ oc new-app --template=system --param WILDCARD_DOMAIN=apps.cluster-$GUID.$GUID.ocp4.opentlc.com
[...]
    deploymentconfig.apps.openshift.io "zync-database" created
--> Success
    Access your application via route '3scale-admin.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Access your application via route 'backend-3scale.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Access your application via route '3scale.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Access your application via route 'api-3scale-apicast-staging.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Access your application via route 'api-3scale-apicast-production.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Access your application via route 'apicast-wildcard.apps.cluster-ede2.ede2.ocp4.opentlc.com'
    Run 'oc status' to view your app.
----

. Verify that no pods were launched:
+
[source]
----
$ oc get pods -n 3scale
No resources found.
----

. Find out why:
+
[source,options="nowrap"]
----
$ oc get events | grep clusterquota
10s         Warning   FailedCreate           DeploymentConfig        Error creating deployer pod: pods "apicast-production-1-deploy" is forbidden: failed quota: clusterquota-andrew: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
1s          Warning   FailedCreate           DeploymentConfig        Error creating deployer pod: pods "apicast-staging-1-deploy" is forbidden: failed quota: clusterquota-andrew: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
[...]
----

[TIP]
`failed quota: ... must specify limits.etc,` etc. means that you forgot to deploy a `LimitRange`.  Try again by redeploying, this time setting the LimitRange.

== Redeploy 3scale with a Limit Range

. Delete the 3scale project to easily start over
+
[source]
----
$ oc delete project 3scale
----

. Create the project again.
+
[source]
----
$ oc new-project 3scale
----

. Create a file called `limits.yaml` with a LimitRange
ifeval::[{show_solution} == true]
+
[source]
----
$ cat >> limits.yaml << EOF
apiVersion: v1
kind: LimitRange
metadata:
  name: 3scale-resource-limits
spec:
  limits:
  - type: Pod
    max:
      cpu: "10"
      memory: 8Gi
    min:
      cpu: 50m
      memory: 100Mi
  - type: Container
    min:
      cpu: 50m
      memory: 100Mi
    max:
      cpu: "10"
      memory: 8Gi
    default:
      cpu: 50m
      memory: 100Mi
    defaultRequest:
      cpu: 50m
      memory: 100Mi
    maxLimitRequestRatio:
      cpu: "200" 
EOF
----
endif::[]
. Deploy a limit range to specify limits on all objects in the 3scale project.
+
[source]
----
$ oc login -u system:admin
$ oc create -f ./limits.yaml
----
. Create the template and deploy the template again:
+
[source]
----
$ oc login -u andrew -p openshift
$ oc create -f ./amp.yml
$ oc new-app --template=system --param WILDCARD_DOMAIN=apps.cluster-$GUID.$GUID.ocp4.opentlc.com
----

[CAUTION]
Now you will encounter a new error. The Quota will not complain about "no limits set". Rather,
deploying a template with ten deployments of one pod exceeds your pod quota.  Why?

. Have a look at the `ClusterResourceQuotas` that are applied in the current context
+
[source]
----
$ oc describe clusterresourcequota
Name:		clusterquota-andrew
Created:	4 minutes ago
Labels:		<none>
Annotations:	<none>
Namespace Selector: ["3scale"]
Label Selector:
AnnotationSelector: map[openshift.io/requester:andrew]
Resource                Used   Hard
--------                ----   ----
configmaps              2      25
limits.cpu              1250m  25
limits.memory           2500Mi 40Gi
persistentvolumeclaims  4      25
pods                    25     25
requests.cpu            1250m  5
requests.memory         2500Mi 6Gi
services                10     25
----

. Find out why all 10 pods aren't deploying correctly:
+
[source]
----
$ oc get events
[...]
2m33s       Warning   FailedCreate             ReplicationController   Error creating: pods "zync-database-1-jc2k4" is forbidden: exceeded quota: clusterquota-andrew, requested: pods=1, used: pods=25, limited: pods=25
2m33s       Warning   FailedCreate             ReplicationController   Error creating: pods "zync-database-1-f9lxc" is forbidden: exceeded quota: clusterquota-andrew, requested: pods=1, used: pods=25, limited: pods=25
2m33s       Warning   FailedCreate             ReplicationController   Error creating: pods "zync-database-1-6vm7b" is forbidden: exceeded quota: clusterquota-andrew, requested: pods=1, used: pods=25, limited: pods=25
[...]
----
+
[TIP]
Examine resource utilization of all the pods with `oc adm top pod`.

. Edit the container requests and limits in your template to restrict CPU and RAM for the `system-app` pod and the other containers in the template so they start reliably.  

[NOTE]
You can experiment with settings by using `oc edit dc` for the various deploymentconfigs for pods that are failing to deploy. Note also the livenessProbe and readinessProbe timeouts, you may need to extend them.

ifeval::[{show_solution} == true]

* This is a portion of the template file showing what needs to change:
+
[source]
----
      containers:
        - env:
          - name: MYSQL_USER
            value: ${MYSQL_USER}
          - name: MYSQL_PASSWORD
            value: ${MYSQL_PASSWORD}
          - name: MYSQL_DATABASE
            value: ${MYSQL_DATABASE}
          - name: MYSQL_ROOT_PASSWORD
            value: ${MYSQL_ROOT_PASSWORD}
          - name: MYSQL_LOWER_CASE_TABLE_NAMES
            value: "1"
          image: ${MYSQL_IMAGE}
          imagePullPolicy: Always
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            tcpSocket:
              port: 3306
          name: system-mysql
          ports:
          - containerPort: 3306
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -i
              - -c
              - MYSQL_PWD="$MYSQL_PASSWORD" mysql -h 127.0.0.1 -u $MYSQL_USER -D $MYSQL_DATABASE
                -e 'SELECT 1'
            initialDelaySeconds: 10
            periodSeconds: 30 <1>
            timeoutSeconds: 5
          resources:
            limits:
              memory: 2Gi # <1>
            requests:
              cpu: "1"
              memory: 1Gi # <1>
----
<1> Decrease these appropriately
endif::[]

ifeval::[{show_solution} == true]

* Here is are template files with a possible solutions:
+
[source,options="nowrap"]
----
$ wget https://raw.githubusercontent.com/newgoliath/ocp_advanced_deployment_assets/master/Managing_Compute_Resources/2_amp_template_solution.yaml
$ wget https://raw.githubusercontent.com/newgoliath/ocp_advanced_deployment_assets/master/Managing_Compute_Resources/3_amp_template_solution.yaml
----
+
* Compare your templates to see in which important ways they differ.
+
[source]
----
$ vimdiff 3_amp_template_solution.yaml 2_amp_template_solution.yaml
----
+
* Find the one you think will work well, and try it out by redeploying as above.

endif::[]

== Clean Up Environment

. Remove the project from your environment:
+
[source]
----
$ oc login -u system:admin
$ oc delete project 3scale
----

. Confirm that you still have cluster quotas in your system:
+
[source]
----
$ oc get clusterresourcequota
----

. Optionally, remove the cluster quota:
+
[source]
----
$ oc delete clusterresourcequota clusterquota-andrew
----

This completes the 3scale API Management lab.
////
https://support.3scale.net/docs/deployment-options/apicast-openshift
https://support.3scale.net/guides/infrastructure/onpremises20-installation
https://www.redhat.com/cms/managed-files/mi-deploying-3scale-api-on-openshift-ebook-f7900-201706-en.pdf
bastion# oc login -u system:admin
bastion# echo "apiVersion: v1
