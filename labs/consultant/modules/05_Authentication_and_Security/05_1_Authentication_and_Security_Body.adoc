include::../../tools/00_0_Lab_Header.adoc[]

== {labname} Lab

.Goals

* Configure TLS certificates for API and default cluster ingress
* Configure authentication and delegate cluster admin privileges
* Expose the container image registry and configure registry storage
* Manage SSH keys for node access

[[labexercises]]
:numbered:

== Configure TLS Certificates

One of the first things to do with a new cluster is to configure TLS certificates for the API and default ingress.
The default certificates are signed by a TLS certificate authority internal to the cluster, which is not acceptable to all customers.
You cannot replace the internal certificates that OpenShift creates and uses, but you can replace the certificate used by the API and default ingress.
In this section of the lab you will use a custom certificate authority to issue certificates and configure these for the cluster.

Your environment has been provisioned with Let's Encrypt certificates.
These are real certificates and the Let's Encrypt certificate authority that creates them is trusted by modern browsers.
You can read more about link:https://letsencrypt.org/getting-started/[Let's Encrypt^] and the link:https://certbot.eff.org/instructions[certbot^] tool used to generate these certificates.

=== Configure API Certificate

To begin, you'll replace the API certificate in your OpenShift cluster that you have deployed.
This is the certificate that is used to secure the external API endpoint into your OpenShift cluster.

NOTE: All of the steps in this lab should be done as your OpenTLC user.
Again, do not be root.
Things will probably not work if you are.

. Set variables for the cluster ingress domain and API hostname.
These are not strictly required, but will help with some of the commands in this lab.
+
[source,sh]
--------------------------------------------------------------------------------
$ ansible localhost -m lineinfile -a 'path=$HOME/.bashrc regexp="^export API_HOSTNAME" line="export API_HOSTNAME='$(oc whoami --show-server | sed -r 's|.*//(.*):.*|\1|')'"'
$ ansible localhost -m lineinfile -a 'path=$HOME/.bashrc regexp="^export INGRESS_DOMAIN" line="export INGRESS_DOMAIN='$(oc get ingresses.config.openshift.io cluster -o jsonpath='{.spec.domain}')'"'
$ source $HOME/.bashrc
--------------------------------------------------------------------------------

. Create a secret, `cluster-apiserver-tls` in the `openshift-config` namespace.
The API TLS secret could be named anything. The name `cluster-apiserver-tls` gives a clear indication of the purpose for this secret.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc create secret tls cluster-apiserver-tls --cert=$HOME/certificates/cert.pem --key=$HOME/certificates/privkey.pem -n openshift-config
--------------------------------------------------------------------------------

. Update the `cluster` API server to use the new certificate.
You may use the `oc patch` command below or make equivalent changes with `oc edit`.
Running this will update the CR and the `openshift-kube-apiserver-operator` will begin deploying new `kubeapiserver` Pods.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc patch apiservers.config.openshift.io cluster --type=merge -p '{"spec":{"servingCerts": {"namedCertificates": [{"names": ["'$API_HOSTNAME'"], "servingCertificate": {"name": "cluster-apiserver-tls"}}]}}}'
--------------------------------------------------------------------------------

. Check the Cluster Operators and wait until the `kube-apiserver` has finished progressing.
This can take several minutes and you will begin to see more and more errors: "Unable to connect to the server: x509: certificate signed by unknown authority".
At some point your oc command will no longer succeed and you will always get the x509 certificate error.
This means that the API servers are now using the new certificate, but your existing `kubeconfig` file still references the old certificate.
+
[source,sh]
--------------------------------------------------------------------------------
$ watch oc get co
--------------------------------------------------------------------------------

. The failure you see in the previous set means that the API is now using the new certificate, but your existing `kubeconfig` file still references the old CA.
Verify that the API is now serving with the new certificate.
+
NOTE: Do not perform this step until you are getting the x509 error 100% of the time in the previous step.
This should take 6-7 minutes.
+
[source,sh]
--------------------------------------------------------------------------------
$ curl https://$API_HOSTNAME:6443/healthz -v

* About to connect() to api.6db0.blue.osp.opentlc.com port 6443 (#0)
*   Trying 169.47.183.32...
* Connected to api.6db0.blue.osp.opentlc.com (169.47.183.32) port 6443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: cacert.pem
  CApath: none
* NSS: client certificate not found (nickname not specified)
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
* 	subject: CN=api.6db0.blue.osp.opentlc.com,OU=GPTE,O=Red Hat,ST=North Carolina,C=US
* 	start date: Dec 03 02:21:10 2019 GMT
* 	expire date: Dec 12 02:21:10 2020 GMT
* 	common name: api.6db0.blue.osp.opentlc.com
* 	issuer: CN=OpenShift Custom Root CA,OU=GPTE,O=Red Hat,L=Raleigh,ST=North Carolina,C=US
> GET /healthz HTTP/1.1
> User-Agent: curl/7.29.0
> Host: api.6db0.blue.osp.opentlc.com:6443
> Accept: */*
>
< HTTP/1.1 200 OK
< Cache-Control: no-cache, private
< Content-Type: text/plain; charset=utf-8
< X-Content-Type-Options: nosniff
< Date: Tue, 03 Dec 2019 02:27:15 GMT
< Content-Length: 2
<
* Connection #0 to host api.6db0.blue.osp.opentlc.com left intact
ok
--------------------------------------------------------------------------------

. The API server is working with the new certificate, but your oc client is still failing.
Try to login with the following command and it will produce an error indicating the certificate is signed by an unknown authority.
Select *No* or *ctrl-c* when prompted to abort the login attempt.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc login -u system:admin

The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n):
--------------------------------------------------------------------------------

. This is due to your `kubeconfig`, which the `oc` client uses, still not being properly updated.
You can update your client `kubeconfig` with the Let's Encrypt CA by running the following command.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc config set-cluster cluster-$GUID --certificate-authority=$HOME/certificates/chain.pem

Cluster $GUID set.
--------------------------------------------------------------------------------

[TIP]
If the `oc config` command above does not work you can just delete every line starting with `certificate-authority-data:` from your kube config files.

. Now attempt to login again and it will succeed.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc login -u system:admin

Logged into "https://api.natedev.red.osp.opentlc.com:6443" as "system:admin" using existing credentials.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
--------------------------------------------------------------------------------

. Your API server is now using a live and Let's Encrypt signed certificate.
+
Your Red Hat Enterprise Linux VM however still doesn't trust the API Server certificate because it does not have the Let's Encrypt root certificate as part of its trusted CA bundle. For that to work you need to once again update the trusted CA bundle.
+
[source,sh]
----
sudo cp ~/certificates/fullchain.pem /etc/pki/ca-trust/source/anchors
sudo update-ca-trust
----
+
From now on you should no longer be prompted with unsigned certificate messages when logging in from the command line on this Virtual Machine.

=== Configure Ingress Default Certificate

In addition to adding a signed certificate for the API server, you can also add a default certificate for ingress.
This means that the certificate will apply for any applications being accessed via your default wildcard ingress route - `*.apps.cluster-$GUID.blue.osp.opentlc.com`.
Again, you will be using a Let's Encrypt certificate that has been provisioned for you.

Most modern browsers already trust the Let's Encrypt root CA, so once you complete this section of the lab, you will not need to do anything further.
Just start up your browser (Chrome or Firefox recommened) and try to access the OpenShift UI.
You should not see any security warnings this time.

. Create a TLS secret to store the default ingress certificate and key in the `openshift-ingress` namespace.
In this lab, you are using the `fullchain.pem` file.
This has both the wildcard certificate as well as the the Let's Encrypt root certificate, removing the need to update the certificate store on all of the hosts in your cluster.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc create secret tls default-ingress-tls --cert=$HOME/certificates/fullchain.pem --key=$HOME/certificates/privkey.pem -n openshift-ingress
--------------------------------------------------------------------------------

. Update the `ingresscontrollers.operator.openshift.io` resource named `default` to use the signed certificate.
Here again, you can use the `oc patch` command below or the equivelent steps using `oc edit`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc patch ingresscontroller.operator default --type=merge -p '{"spec":{"defaultCertificate": {"name": "default-ingress-tls"}}}' -n openshift-ingress-operator
--------------------------------------------------------------------------------

. Watch for the operator to restart the default router pods in the `openshift-ingress` namespace.
When you change something, such as a `secret` or `configmap`, the Pods have to be restarted to consume the new or updated artifacts.
+
[source,sh]
--------------------------------------------------------------------------------
$ watch oc get pod -n openshift-ingress
--------------------------------------------------------------------------------

. You can also watch the `oc get co` output to see some of the other cluster operators cycle.
It will take approximately 5 minutes for this to complete.
+
[source,sh]
--------------------------------------------------------------------------------
$ watch oc get co
--------------------------------------------------------------------------------

. Verify Ingress router certificate by using the `curl` command to validate the console route.
+
[source,sh]
--------------------------------------------------------------------------------
$ curl $(oc whoami --show-console) -v | head -1

* About to connect() to console-openshift-console.apps.6db0.blue.osp.opentlc.com port 443 (#0)
*   Trying 169.47.183.36...
* Connected to console-openshift-console.apps.6db0.blue.osp.opentlc.com (169.47.183.36) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /home/nstephan-redhat.com/ca/cacert.pem
  CApath: none
* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* Server certificate:
* 	subject: CN=apps.6db0.blue.osp.opentlc.com,OU=GPTE,O=Red Hat,ST=North Carolina,C=US
* 	start date: Dec 03 02:22:54 2019 GMT
* 	expire date: Dec 12 02:22:54 2020 GMT
* 	common name: apps.6db0.blue.osp.opentlc.com
* 	issuer: CN=OpenShift Custom Root CA,OU=GPTE,O=Red Hat,L=Raleigh,ST=North Carolina,C=US
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: console-openshift-console.apps.6db0.blue.osp.opentlc.com
> Accept: */*
>
< HTTP/1.1 200 OK
...
--------------------------------------------------------------------------------

Your ingress controllers, or routers, are now configured to use a signed wild-card certificate for routes configured to use `reencrypt` or `edge` termination.

== Delegating Cluster Admin Rights

You should already be familiar with the basics of configuring OpenShift authentication and role-based access control from the prerequisites for this training.
In this section you will review configuring an OAuth provider, configuring a group, and delegating cluster admin privileges.
Finally you will disable the kubeadmin user.

=== Configure Local Password Identity Provider

In this step you will configure authentication with local passwords using the `htpasswd` provider.
In a real world cluster you would normally integrate with single-sign-on (SSO) or LDAP for user authentication.

. To begin, you will need to create an `htpasswd` secret.
Start by creating the `htpasswd` file with usernames and hashed passwords.
Use the `htpasswd` command to create a file called `$HOME/htpasswd` with a user named `andrew` and a password of `openshift`.
+
[source,sh]
--------------------
$ cd $HOME
$ touch $HOME/htpasswd
$ htpasswd -Bb $HOME/htpasswd andrew openshift
--------------------

. Repeat the required part of the previous step to create users `david` and `karla`.

. Create the `htpasswd` secret from the `htpasswd` file in the `openshift-config` namespace.
The name of the data key within the secret must be `htpasswd`.
In this example you created a file with the same name, so the data key name for the secret need not be given explicitly.
+
[source,sh]
-----------------------------------------------------------------------------
$ oc create secret generic htpasswd --from-file=$HOME/htpasswd -n openshift-config
-----------------------------------------------------------------------------

. Configure an `HTPasswd` identity provider with name "Local Password" that authenticates users using the `htpasswd` data stored in the secret you just created.
+
[source,sh]
----
$ oc apply -f - <<EOF
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: Local Password
    mappingMethod: claim
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpasswd
EOF
----

. As you have seen with previous examples in this lab, when you modify a CR, the `operator` that is responsible for it will take action.
In this case, the `oauth-openshift` Pods will restart in the `openshift-authentication` namespace to pick up the new configuration.
+
[source,sh]
--------------------------------------------------------------------------------
$ watch oc get pod -n openshift-authentication
--------------------------------------------------------------------------------

. Attempt login as one of the users you just created.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc login -u andrew -p openshift $(oc whoami --show-server)

Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project <projectname>
--------------------------------------------------------------------------------

. After verifying that local password authentication worked, return to using the `system:admin` user profile from your `kubeconfig`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc login -u system:admin
--------------------------------------------------------------------------------

You have now set up an identity provider in your OpenShift cluster. 
The process is similar for all supported identity providers, but the prerequisites will be different.
See more details about the link:https://docs.openshift.com/container-platform/4.2/authentication/understanding-identity-provider.html#supported-identity-providers[supported identity providers^] in the OpenShift documentation.

=== Delegate cluster-admin privileges

In this section, you will create a new user group and then grant the `cluster-admin` cluster role binding to the group.
The `cluster-admin` role is the most powerful role in your OpenShift cluster and will allow a user to do anything inside of the cluster.
Membership in a group bound to this role should be given extremely sparingly.

. Create a group named `lab-cluster-admins` with members `david` and `karla`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc adm groups new lab-cluster-admins david karla
--------------------------------------------------------------------------------

. Grant the `cluster-admin` cluster role to the `lab-cluster-admins` group.
Do this by creating a `clusterrolebinding` and associating the cluster role with the group.
While providing the `rolebinding-name` is optional, setting it makes it easier to validate cluster access.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc adm policy add-cluster-role-to-group cluster-admin lab-cluster-admins --rolebinding-name=lab-cluster-admins
--------------------------------------------------------------------------------

. Log in as one of the members of the `lab-cluster-admins` group and verify cluster admin level permissions.
The command `oc auth can-i <verb> <resource>` is useful to check access.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc login -u karla -p openshift $(oc whoami --show-server)
$ oc auth can-i delete node
--------------------------------------------------------------------------------

=== Disable the kubeadmin user

The `kubeadmin` user account is meant to be used initially after cluster install to manage the cluster.
It is best practice to disable `kubeadmin` access soon after other authentication methods have been granted and cluster admin rights are delegated to other users.
The `system:admin` user authenticates with TLS certificates and continues to function after `kubeadmin` is disabled.
However, you cannot log into the console UI with the `system:admin` user.
You must login with a user controlled by the identity provider you configured.

. Disable the `kubeadmin` by deleting the `kubeadmin` secret from the `kube-system` namespace.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc delete secret kubeadmin -n kube-system
--------------------------------------------------------------------------------

== Configure the Container Image Registry

In this section you will be exposing the OpenShift integrated container image registry and creating a service account with rights to push images into the registry.
This is another common task that you may need to perform in an OpenShift cluster.
While some customers have existing enterprise registries that they intend to primarily use, that does not mean they won't want to use the integrated OpenShift registry as well.
The solutions are not mutually exclusive.

The OpenShift registry has some added benefits with regards to security such as obeying the RBAC you have configured in your cluster.
Any image you push to the OpenShift registry in your project will only be accessible by users with the proper access in your project.

=== Expose the integrated container image registry

By default, the OpenShift integrated registry service is not exposed.
This means while it is usable inside the cluster, it is not accessible from the outside.
Fortunately, OpenShift 4 makes this change quite easy.
You simply update the CR and the image registry Operator does the rest.

. Begin by exposing the image registry with the default route.
Setting `spec.defaultRoute` to `true` in the `configs.imageregistry.operator.openshift.io` custom resource named `cluster` causes the cluster-image-registry-operator to create a route to expose the registry.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge --patch '{"spec":{"defaultRoute":true}}'
--------------------------------------------------------------------------------

. Check the route that was created for you by the Operator.
The route termination is "reencrypt", meaning that the default TLS certificate will be used to expose the route through the cluster ingress routers.
The default route uses a rather long name prefix, "default-route-openshift-image-registry".
+
[source,sh]
--------------------------------------------------------------------------------
$ oc get route -n openshift-image-registry

NAME            HOST/PORT                                                               PATH   SERVICES         PORT    TERMINATION   WILDCARD
default-route   default-route-openshift-image-registry.apps.d12e.blue.osp.opentlc.com          image-registry   <all>   reencrypt     None
--------------------------------------------------------------------------------

. Create a route with a shorter, easier to use, hostname.
Routes may also be added by the operator by adding entries to the list `spec.routes` with keys `name` and `hostname`.
Create a route through the operator with name `image-registry` and the hostname `image-registry.$INGRESS_DOMAIN`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge --patch '{"spec":{"routes":[{"name":"image-registry", "hostname":"image-registry.'$INGRESS_DOMAIN'"}]}}'
--------------------------------------------------------------------------------

. Verify that you now have two routes for the image registry.
Both should be pointing to the same service, so either can be used.

. Check the image-registry route using `curl`.
The registry should respond with a "200 OK" response with no content.
This works without specifying a `cacert` or `-k` because this is using the default TLS certificate you configured earlier in the lab.
+
[source,sh]
--------------------------------------------------------------------------------
$ curl https://$(oc get route -n openshift-image-registry image-registry -o jsonpath='{.spec.host}')/healthz -v
--------------------------------------------------------------------------------

=== Configure a service account to push images to the registry

Automation and build tools often need to push images into the cluster registry to make them available for applications and cluster infrastructure.
In this section you will create a service account and configure it with rights to push to the registry.
You'll then push a test image to the OpenShift integrated registry.

. Create a `registry-admin` service account in the `openshift-config` namespace.
The namespace used for this service account is somewhat arbitrary and you could create it elsewhere.
The `openshift-config` namespace is recommended because it is a location where a number of other cluster configurations are stored.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc create serviceaccount registry-admin -n openshift-config
--------------------------------------------------------------------------------

. Grant the `registry-admin` cluster role to the `registry-admin` service account.
The pattern to do this should be familiar from your previous exercise where you granted a different cluster role to a user.
The difference here is the user is a service account and is referenced differently.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc adm policy add-cluster-role-to-user registry-admin system:serviceaccount:openshift-config:registry-admin
--------------------------------------------------------------------------------

. Create the ImageStream, `ubi8` in the `openshift` namespace.
This is a special namespace and will make the ImageStream available to other projects in the cluster.
While the topic of `ImageStreams` is outside the scope of the lab, you can read about them in link:https://docs.openshift.com/container-platform/4.2/openshift_images/image-streams-manage.html[this section^] of the OpenShift documentation.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc create imagestream ubi8 -n openshift
--------------------------------------------------------------------------------

. Install `skopeo` with yum onto the bastion host.
`Skopeo` is a tools that lets you perform various tasks on container images, such as copying images between different registries or storage mechanisms.
+
[source,sh]
--------------------------------------------------------------------------------
$ sudo yum install -y skopeo
--------------------------------------------------------------------------------

. Verify `registry-admin` can push to the `ubi8` ImageStream in the `openshift` namespace.
The `REGISTRY_ADMIN_TOKEN` and `UBI8_IMAGE_REPO` variables are being set to help with the command - they are not required.
You will use `skopeo copy` to copy `docker://registry.access.redhat.com/ubi8:latest` to the `ubi8` ImageStream in the `openshift` namespace using tag `latest`.
+
[source,sh]
--------------------------------------------------------------------------------
$ REGISTRY_ADMIN_TOKEN=$(oc sa get-token -n openshift-config registry-admin)
$ UBI8_IMAGE_REPO="image-registry.$INGRESS_DOMAIN/openshift/ubi8"
$ skopeo copy docker://registry.access.redhat.com/ubi8:latest docker://$UBI8_IMAGE_REPO:latest --dest-creds -:$REGISTRY_ADMIN_TOKEN
--------------------------------------------------------------------------------
+
NOTE: With this command, a username must be provided but the value is ignored.
A common convention is to pass username "-" or "unused".

. You can verify this image was pushed to the OpenShift integrated registry by trying to pull it using `podman`.
+
[source,sh]
--------------------------------------------------------------------------------
$ podman pull $UBI8_IMAGE_REPO:latest --creds -:$REGISTRY_ADMIN_TOKEN
$ podman images

REPOSITORY                                                     TAG                          IMAGE ID       CREATED       SIZE
image-registry.apps.d12e.blue.osp.opentlc.com/openshift/ubi8   latest                       096cae65a207   5 weeks ago   239 MB
utilityvm.example.com:5000/ocp4/openshift4                     operator-lifecycle-manager   b36f3475d666   6 weeks ago   403 MB
--------------------------------------------------------------------------------

== Configure SSH access to nodes

When you installed your cluster you had the option of providing an SSH key to use for access to the nodes.
While you should not use this access to configure nodes, it can be useful when troubleshooting issues.
In this section we will look at how SSH keys are configured for node access and how to use SSH keys to access nodes.

=== Explore the SSH key configuration in MachineConfig

In this exercise you create a Pod running in your cluster with SSH access to the nodes.
In the OpenStack environment you are using, the bastion host is able to make direct SSH connections to the nodes.
In many cloud providers and more isolated networks, the topology or firewalls might prevent this direct access.
If your nodes are on a routed network that you have access to or you have a jump host like you do with your bastion, this part of the lab would be unecessary.

. Inspect the MachineConfig resources configured during installation.
You will see that there are two for SSH, `99-master-ssh` and `99-worker-ssh` used for SSH access to the masters and workers respectively.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc get machineconfigs.machineconfiguration.openshift.io

NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
00-worker                                                   d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
01-master-container-runtime                                 d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
01-master-kubelet                                           d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
01-worker-container-runtime                                 d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
01-worker-kubelet                                           d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
99-master-abd6a63f-389c-11ea-8faa-fa163ef02d2b-registries   d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
99-master-ssh                                                                                          2.2.0             5h54m
99-worker-abdcc360-389c-11ea-8faa-fa163ef02d2b-registries   d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
99-worker-ssh                                                                                          2.2.0             5h54m
rendered-master-dd3a7544f2c0bda48bd8b41e5110d9b3            d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
rendered-worker-3d5f2ffcd5a16019f8d2225ed3e8e54a            d780d197a9c5848ba786982c0c4aaa7487297046   2.2.0             5h54m
--------------------------------------------------------------------------------

. Take a look at the definition of the `99-worker-ssh` MachineConfig.
Note the value of `spec.config.passwd.users` includes a user named `core` with a value for `sshAuthorizedKeys`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc get machineconfig 99-worker-ssh -o yaml

kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-ssh
spec:
  config:
    ignition:
      config: {}
      security:
        tls: {}
      timeouts: {}
      version: 2.2.0
    networkd: {}
    passwd:
      users:
      - name: core
        sshAuthorizedKeys:
        - |
          ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDjmgPlnc9RT25ecYhYmXUBrvkA1pfZdb2swdeWYlFm3APr8Fn8CAZCBx82SK/UQfkzgWKWNH2BSaO/SAZmtzWtJnQg0MeHS7l3111FI7wUkRZMAHt8BSiJZpogdN9epMJKeXL9a0UI89ZsJpFwzbG7gDSTppn8m28BmYHQOTqBOU34bUYJ92CLFQ5VcjXjRPBdCTf/5k9x3dkUgUfXp/5i8k9EJR0L9ntQjZyvmDm5gqWoEQydwAkTI5r7JUiKy3KaaoNKcK/9f9TrFqfEyY7PHq0/vWkN3iXGqP1vkvwkwE9kuHMJK6AyxOEKDPV/M8QFtpkKsDM4C71QZmdMzdIR opentlc-mgr@admin.na.shared.opentlc.com
    storage: {}
    systemd: {}
  osImageURL: ""
--------------------------------------------------------------------------------

. Create a new project namespace, `node-ssh` for node SSH access configuration.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc new-project node-ssh
--------------------------------------------------------------------------------

. Create an application BuildConfig and ImageStream, both with name `node-ssh`.
Use the `ubi8` ImageStream from the `openshift` namespace as the base image and use a docker strategy build to install the `openssh-clients` package.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc new-build openshift/ubi8:latest --name=node-ssh --dockerfile - <<EOF
FROM unused
RUN dnf install -y openssh-clients
CMD ["sleep", "infinity"]
EOF
--------------------------------------------------------------------------------

. The build will start automatically.
Watch for build completion (it will take a few seconds for the build pod to start):
+
[source,sh]
--------------------------------------------------------------------------------
$ oc logs -f node-ssh-1-build
--------------------------------------------------------------------------------

. Now that you have a container image that you can use to SSH into, proceed with creating a Deployment.
Start by creating a secret also named `node-ssh` containing the cluster SSH private key as `id_rsa`.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc create secret generic node-ssh --from-file=id_rsa=$HOME/.ssh/${GUID}key.pem -n node-ssh
--------------------------------------------------------------------------------

. Start the Deployment definition using `oc create deployment`, naming it `node-ssh` and using the new image from the `node-ssh` ImageStream.
Instead of deploying this immediately, you will use the `--dry-run` option and save the output so that you can make some changes before deploying.
+
[source,sh]
--------------------------------------------------------------------------------
$ NODE_SSH_IMAGE=$(oc get imagestream node-ssh -o jsonpath='{.status.dockerImageRepository}' -n node-ssh)

$ oc create deployment node-ssh --image=$NODE_SSH_IMAGE:latest --dry-run -o yaml -n node-ssh > $HOME/node-ssh.deployment.yaml
--------------------------------------------------------------------------------

. Edit `$HOME/node-ssh.deployment.yaml` to mount the `node-ssh` secret at `/.ssh` within the container.
The resulting deployment definition should look like this.
+
[source,sh]
--------------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: node-ssh
  name: node-ssh
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-ssh
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: node-ssh
    spec:
      containers:
      - image: image-registry.openshift-image-registry.svc:5000/node-ssh/node-ssh:latest
        name: node-ssh
        resources: {}
        volumeMounts:
        - name: node-ssh
          mountPath: /.ssh
      volumes:
      - name: node-ssh
        secret:
          secretName: node-ssh
          defaultMode: 0600
status: {}
--------------------------------------------------------------------------------

. Use `oc apply` to create the deployment:
+
[source,sh]
--------------------------------------------------------------------------------
$ oc apply -f $HOME/node-ssh.deployment.yaml
--------------------------------------------------------------------------------

. Once your new Pod is deployed, you can proceed with testing SSH access to a node.
Start by retrieving a list of worker nodes in your cluster.
The `-o wide` option will show the internal IP address of the node.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc get node -l node-role.kubernetes.io/worker -o wide
--------------------------------------------------------------------------------

. Using one of the IP addresses, attempt SSH access to the node, using the `node-ssh` pod and authenticating as user `core`.
+
[source,sh]
--------------------------------------------------------------------------------
$ NODE_SSH_POD=$(oc get pod -l app=node-ssh -o jsonpath='{.items[0].metadata.name}')

$ oc exec -it $NODE_SSH_POD -- ssh core@<your-node-IP>
--------------------------------------------------------------------------------

. Accept the ECDSA key fingerprint when prompted.
.. Use the `uptime` command to check how long the node has been available.
.. See all of the `CRI-O` containers running with `crictl ps`.
.. Since that fails, try running with `sudo` to see that the `core` user has full root access.

. Logout of the node when finished by typing `exit`.

=== Add an SSH key for worker machine access

In this section you will add a new SSH key to the worker and master `MachineConfigs`.
A similar pattern could be used to rotate SSH keys using `MachineConfigs`.

. Create a new SSH key pair.
Save the private key as `$HOME/.ssh/node.id_rsa` and do not set a passphrase.
+
[source,sh]
--------------------------------------------------------------------------------
$ ssh-keygen -t rsa -f $HOME/.ssh/node.id_rsa -N ''
--------------------------------------------------------------------------------

. Display the new public key to ensure it was created correctly.
+
[source,sh]
--------------------------------------------------------------------------------
$ cat $HOME/.ssh/node.id_rsa.pub
--------------------------------------------------------------------------------

. Edit the `99-worker-ssh` MachineConfig and add the public key to the user `core`.
You can use the `oc patch` command below or the equivelent `oc edit` steps.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc patch machineconfig 99-worker-ssh --type=json --patch="[{\"op\":\"add\", \"path\":\"/spec/config/passwd/users/0/sshAuthorizedKeys/-\", \"value\":\"$(cat $HOME/.ssh/node.id_rsa.pub)\"}]"
--------------------------------------------------------------------------------

. Do the same for the `99-master-ssh` MachineConfig.
+
WARNING: You may briefly lose your connection as some of the master nodes cycle.
It will come back.

. Wait for the MachineConfig update to deploy.
A short time after updating the worker configuration the machine config daemon will begin restarting the nodes in the cluster as part of the reconfiguration. It may take 15 minutes or more to cycle through the nodes. Your nodes will show *SchedulingDisabled*, *NotReady* and finally *Ready*. One after the other.
+
[source,sh]
--------------------------------------------------------------------------------
watch oc get nodes
--------------------------------------------------------------------------------

. Replace the `node-ssh` secret you created earlier and rollout a new instance of the `node-ssh` Pod.
The existing Pod won't get the updated Secret if you do not redeploy the pod.
Alternately, you can try to SSH directly from the bastion to one of the nodes using the new SSH key as well.
+
Reset the `node-ssh` deployment to use the new key.
+
[source,sh]
--------------------------------------------------------------------------------
$ oc delete secret node-ssh -n node-ssh

$ oc create secret generic node-ssh --from-file=id_rsa=$HOME/.ssh/node.id_rsa -n node-ssh

$ oc delete pod -l app=node-ssh -n node-ssh
--------------------------------------------------------------------------------

. To verify that the new SSH key is in place, test it using the same process you used earlier with the `node-ssh` Pod.
+
[source,sh]
--------------------------------------------------------------------------------
$ NODE_SSH_POD=$(oc get pod -l app=node-ssh -o jsonpath='{.items[0].metadata.name}' -n node-ssh)

$ oc exec -it $NODE_SSH_POD -- ssh core@<your-node-IP>
--------------------------------------------------------------------------------

. After connecting to the node, check `uptime`.
Observe that the node has rebooted recently.

== Summary

In this lab, you accomplished several things.

First, you replaced your self-signed API and default ingress certificates with a new signed certificate from Let's Encrypt. This allows you to connect with both the API and any applications accessed via the `*.apps.cluster-$GUID.blue.osp.opentlc.com` domain without having to accept a self-signed certificate. This will be a common requirement with customers and they will often want to use their own PKI systems to generate a signed certificate that you must use.

Following the certificate replacement, you updated the identity provider in your cluster. While there are several to choose from, in this lab you used the `HTPasswd` identity provider. With the users you created, you assigned necessary cluster roles to give them access to do things. Wrapping up this section, you removed the `kubeadmin` account, which is really set up as a convenience for you to initially access the cluster after building.

The OpenShift registry was next in this lab. You exposed the registry outside of the cluster so that it can be directly accessed using tools such as `podman` or `skopeo`.

Finally, you updated some of the machine configs on the nodes in your cluster. You managed the SSH keys for all of the nodes in your cluster _from_ OpenShift. Previously, you would have had to write automation or do all of this manually.

In OpenShift 4, you declare what you want the configuration to be and the platform (along with the Operators) go and do it.

== Solving and Resetting (beta)

Use the steps here to either solve or reset this lab.
Solving the lab will set up everything that you would have done in this lab.

There is no reset option for this lab.

. Run the solver using FTL.
+
[source,options="nowrap"]
----
$ ansible-playbook /opt/ftl-repo-clone/courses/ocp4_advanced_deployment/lab_05_1/solve_lab.yml
----
